{
	"name": "mysql_import_incremental__1__loader",
	"properties": {
		"description": "Well, this one uses a Filter condition for making things work for incremental loads.\n\nWARNING: \"tables\" object must be type ARRAY.\nAll objects in this array must be, in reality STRING type, enclosed by \".\nInside this objects, you should enclose everything in SINGLE QUOTES.\nOtherwise, things are not going to work. I warned you!\n\nIMPORTANT: MySQL Only supports REGEXP_REPLACE() on version 8.0+. IF you are using an older version, apply this using admin privileges:\n\nSET sql_mode = 'NO_BACKSLASH_ESCAPES';\n\nDELIMITER $$\n CREATE FUNCTION `regexp_replace`(original VARCHAR(1000), pattern VARCHAR(1000),replacement VARCHAR(1000))\nRETURNS VARCHAR(1000)\n DETERMINISTIC\n BEGIN \n  DECLARE temp VARCHAR(1000); \n  DECLARE ch VARCHAR(1); \n  DECLARE i INT;\n  SET i = 1;\n  SET temp = '';\n  IF original REGEXP pattern THEN \n   loop_label: LOOP \n    IF i>CHAR_LENGTH(original) THEN\n     LEAVE loop_label;  \n    END IF;\n    SET ch = SUBSTRING(original,i,1);\n    IF NOT ch REGEXP pattern THEN\n     SET temp = CONCAT(temp,ch);\n\n   ELSE\n     SET temp = CONCAT(temp,replacement);\n    END IF;\n    SET i=i+1;\n   END LOOP;\n  ELSE\n   SET temp = original;\n  END IF;\n  RETURN temp;\n END$$\n DELIMITER ;\n\n\n\n\n",
		"activities": [
			{
				"name": "filter_control_column_type_2_db_datetime",
				"description": "Filtering control column type as int, if parameter \"control_column_type_2_db\" in table dictionary is datetime.\n\nThis parameter \"control_column_type_2_db\" isn't the original type from database, but rather the behavior of the data. So it could be a string, int or datetime. It will work only if the column represents a datetime format.",
				"type": "Filter",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@pipeline().parameters.tables",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).control_column_type_2_db), 'datetime')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "1_for_each_incremental_control_column_datetime",
				"description": "This makes our pipeline generic and iterable. After filtering the incremental tables that have the control column that behaves like a datetime and you want to load all the data until the current datetime (not by year), we shall proceed working with on any of them . \n\nAfter passing through this loop, \"tables\" pipeline parameter must be casted to json when needed.\n\nThis implementation inherits from Oracle's implementation. Any column that can be interpreted as datetime might be loaded. ",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "filter_control_column_type_2_db_datetime",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_control_column_type_2_db_datetime').output.value\n",
						"type": "Expression"
					},
					"isSequential": false,
					"batchCount": 4,
					"activities": [
						{
							"name": "1_load_delta_incremental_table",
							"description": "Loading the incremental delta requires a lowerbound and an upperbound. \nUpperbound was generated by the lookup activity \"1_get_max_control_column_in_source_db\". And lowerbound was generated by the lookup activity \"1_get_watermark_row_for_table\".\n\nIn order to be able to receive control column as integer or datetimes, we generalized converting to timestamp, then to string and then to int.\n\nWe had so many trouble with some string columns, so we decided to use REGEX and TRIM to ensure the we trim all the characters that aren't STRICTLY NUMBERS or elements of datetime such as \":\", \"/\", \"-\" and whitespace.\n\nWARNING: All the limits must be inclusive, that is, the the control column should be => lowerbound and <= upperbound!! It's possible that we are getting redundant compared to the last load, because all the limits are inclusive. Don't worry, it will be considered in the processing pipeline with Spark.",
							"type": "Copy",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 2,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "MySqlSource",
									"query": {
										"value": "SELECT @{json(item()).columns} FROM @{json(item()).schema}.@{json(item()).table} WHERE CAST(COALESCE(DATE_FORMAT(TRIM(REGEXP_REPLACE(@{json(item()).control_column}, '[^0-9:\\/\\s-]', '' )), '@{json(item()).control_column_mask_value}'), '@{pipeline().parameters.increment.control_column.lowerbound}') AS UNSIGNED) >= CAST('@{pipeline().parameters.increment.control_column.lowerbound}' AS UNSIGNED) AND CAST(COALESCE(DATE_FORMAT(TRIM(REGEXP_REPLACE(@{json(item()).control_column}, '[^0-9:\\/\\s-]', '' )), '@{json(item()).control_column_mask_value}'), '@{pipeline().parameters.increment.control_column.lowerbound}') AS UNSIGNED) <= @{pipeline().parameters.increment.control_column.upperbound}\n",
										"type": "Expression"
									}
								},
								"sink": {
									"type": "ParquetSink",
									"storeSettings": {
										"type": "AzureBlobFSWriteSettings"
									},
									"formatSettings": {
										"type": "ParquetWriteSettings"
									}
								},
								"enableStaging": false,
								"enableSkipIncompatibleRow": true,
								"redirectIncompatibleRowSettings": {
									"linkedServiceName": {
										"referenceName": "cnibigdatadlsgen2stg",
										"type": "LinkedServiceReference"
									},
									"path": {
										"value": "@{pipeline().parameters.container}/@{pipeline().parameters.dls.folders.error}/dbo/@{json(item()).schema}/@{json(item()).table}",
										"type": "Expression"
									}
								}
							},
							"inputs": [
								{
									"referenceName": "mysql_table_parameterized",
									"type": "DatasetReference"
								}
							],
							"outputs": [
								{
									"referenceName": "adls_parameterized_partitioned_source",
									"type": "DatasetReference",
									"parameters": {
										"container": {
											"value": "@pipeline().parameters.container",
											"type": "Expression"
										},
										"url": {
											"value": "@pipeline().parameters.url",
											"type": "Expression"
										}
									}
								}
							]
						},
						{
							"name": "1_run_raw_notebook",
							"description": "Invokes the pipeline for running Databricks",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "1_load_delta_incremental_table",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__4__run_databricks_notebook",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"databricks": {
										"value": "@pipeline().parameters.databricks",
										"type": "Expression"
									},
									"adf": {
										"value": "@pipeline().parameters.adf",
										"type": "Expression"
									},
									"dls": {
										"value": "@pipeline().parameters.dls",
										"type": "Expression"
									},
									"table": {
										"value": "@string(item())",
										"type": "Expression"
									}
								}
							}
						},
						{
							"name": "1_raw_load_dbo_unified__5_update_watermark_in_db",
							"description": "Updates the watermark in watermark's database.",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "1_run_raw_notebook",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__5__update_watermark_in_db",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"watermark": {
										"value": "@pipeline().parameters.watermark",
										"type": "Expression"
									},
									"increment": {
										"value": "@pipeline().parameters.increment",
										"type": "Expression"
									},
									"table": {
										"value": "@json(item())",
										"type": "Expression"
									}
								}
							}
						}
					]
				}
			}
		],
		"parameters": {
			"db": {
				"type": "object"
			},
			"tables": {
				"type": "array"
			},
			"dls": {
				"type": "object"
			},
			"watermark": {
				"type": "object"
			},
			"databricks": {
				"type": "object"
			},
			"adf": {
				"type": "object"
			},
			"increment": {
				"type": "object"
			},
			"container": {
				"type": "string"
			},
			"url": {
				"type": "string"
			}
		},
		"folder": {
			"name": "templates/raw/bdo/mysql"
		},
		"annotations": [
			"template",
			"raw",
			"mysql"
		]
	},
	"type": "Microsoft.DataFactory/factories/pipelines"
}