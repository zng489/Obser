{
	"name": "mysql_import_full",
	"properties": {
		"description": "Well, this one uses a Filter condition for making thing work for full loads.\n\nMySQL connector does not support partitioning.\n\nWARNING: the tables object must be type ARRAY.\nAll objects in this array must be, in reality STRING type, enclosed by \".\nInsisde this objects, you should enclose everything in SINGLE QUOTES.\nOtherwise, things are not going to work. I warned you!\n\nHere's an example:\n[\"{'schema': 'systemx', 'table':'customers','load_type':'full', 'raw': {'coalesce': 1}}\"]",
		"activities": [
			{
				"name": "filter_tables_load_type_is_full",
				"description": "Using filters and a definition of the table like an object, we must be able to get things going on a flux which separates incremental and full load. ",
				"type": "Filter",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@pipeline().parameters.tables",
						"type": "Expression"
					},
					"condition": {
						"value": "@or(equals(toLower(json(item()).load_type), 'full'), equals(toLower(json(item()).load_type), 'full_balance'))",
						"type": "Expression"
					}
				}
			},
			{
				"name": "for_each_table_load_type_full",
				"description": "MySQL does not support partitions when copying data. \n\nThis makes our pipeline generic and iterable. After filtering the full tables, we shall proceed working with on any of them . \n\nAfter passing through this loop, \"tables\" pipeline parameter must be casted to json when needed.\n\nAnd don't worry; if output is null, the rest of the flow is not executed! At least this!",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "filter_tables_load_type_is_full",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_tables_load_type_is_full').output.value",
						"type": "Expression"
					},
					"isSequential": false,
					"batchCount": 4,
					"activities": [
						{
							"name": "1_load_full_table",
							"description": "To load the table without partitioning, just execute a simple select * from table. No need to calculate upper bound, so we don't need the activity \"get_max_partition_column_in_source_db\". ",
							"type": "Copy",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 2,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "MySqlSource",
									"query": {
										"value": "SELECT @{json(item()).columns} FROM @{json(item()).schema}.@{json(item()).table}",
										"type": "Expression"
									}
								},
								"sink": {
									"type": "ParquetSink",
									"storeSettings": {
										"type": "AzureBlobFSWriteSettings"
									},
									"formatSettings": {
										"type": "ParquetWriteSettings"
									}
								},
								"enableStaging": false,
								"parallelCopies": 4,
								"enableSkipIncompatibleRow": true,
								"redirectIncompatibleRowSettings": {
									"linkedServiceName": {
										"referenceName": "cnibigdatadlsgen2stg",
										"type": "LinkedServiceReference"
									},
									"path": {
										"value": "@{pipeline().parameters.container}/@{pipeline().parameters.dls.folders.error}/dbo/@{json(item()).schema}/@{json(item()).table}",
										"type": "Expression"
									}
								},
								"dataIntegrationUnits": 2
							},
							"inputs": [
								{
									"referenceName": "mysql_private_endpoint_parameterized",
									"type": "DatasetReference"
								}
							],
							"outputs": [
								{
									"referenceName": "adls_parameterized_partitioned_source",
									"type": "DatasetReference",
									"parameters": {
										"container": {
											"value": "@pipeline().parameters.container",
											"type": "Expression"
										},
										"url": {
											"value": "@pipeline().parameters.url",
											"type": "Expression"
										}
									}
								}
							]
						},
						{
							"name": "1_run_raw_notebook",
							"description": "Invokes the pipeline for running Databricks",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "1_load_full_table",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__4__run_databricks_notebook",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"databricks": {
										"value": "@pipeline().parameters.databricks",
										"type": "Expression"
									},
									"adf": {
										"value": "@pipeline().parameters.adf",
										"type": "Expression"
									},
									"dls": {
										"value": "@pipeline().parameters.dls",
										"type": "Expression"
									},
									"table": {
										"value": "@string(item())",
										"type": "Expression"
									}
								}
							}
						}
					]
				}
			}
		],
		"parameters": {
			"db": {
				"type": "object"
			},
			"tables": {
				"type": "array"
			},
			"dls": {
				"type": "object"
			},
			"watermark": {
				"type": "object"
			},
			"databricks": {
				"type": "object"
			},
			"adf": {
				"type": "string"
			},
			"container": {
				"type": "string"
			},
			"url": {
				"type": "string"
			}
		},
		"folder": {
			"name": "templates/raw/bdo/mysql"
		},
		"annotations": [
			"raw",
			"template",
			"mysql"
		]
	},
	"type": "Microsoft.DataFactory/factories/pipelines"
}