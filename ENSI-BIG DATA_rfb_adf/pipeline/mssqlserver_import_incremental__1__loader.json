{
	"name": "mssqlserver_import_incremental__1__loader",
	"properties": {
		"description": "Well, this one uses a Filter condition for making things work for incremental loads.\n\nWARNING: \"tables\" object must be type ARRAY.\nAll objects in this array must be, in reality STRING type, enclosed by \".\nInside this objects, you should enclose everything in SINGLE QUOTES.\nOtherwise, things are not going to work. I warned you!\n\nFor this template to work, you must implement the following function on SQLServer database.\n\nCREATE OR ALTER FUNCTION [dbo].[REGEXP_REPLACE]\n(\n    @String NVARCHAR(MAX), \n    @MatchExpression VARCHAR(255), \n    @FormatoEntrada VARCHAR(19)\n)\n/* criado em 2021-06-12 por raionan (encontrado em https://stackoverflow.com/questions/1007697/how-to-strip-all-non-alphabetic-characters-from-string-in-sql-server)\n-- modo de uso nome anterior fn_StripCharacters\n-- Alphabetic only:\nSELECT dbo.fn_StripCharacters('a1!s2@d3#f4$', '^a-z')\n-- Numeric only:\nSELECT dbo.fn_StripCharacters('a1!s2@d3#f4$', '^0-9')\n-- Alphanumeric only:\nSELECT dbo.fn_StripCharacters('a1!s2@d3#f4$', '^a-z0-9')\n-- Non-alphanumeric:\nSELECT dbo.fn_StripCharacters('a1!s2@d3#f4$', 'a-z0-9')\n*/\nRETURNS NVARCHAR(MAX)\nAS\nBEGIN\n    if upper(@FormatoEntrada) = 'MMYYYY'\n        set @String = right(@String,4)+left(@String,2)+'01000000'\n        \n        \n    SET @MatchExpression =  '%['+@MatchExpression+']%'\n    WHILE PatIndex(@MatchExpression, @String) > 0\n        SET @String = Stuff(@String, PatIndex(@MatchExpression, @String), 1, '')\n    \n    if upper(@FormatoEntrada) = 'YYYYMM'\n        set @String = @String+'01000000'\n    if upper(@FormatoEntrada) = 'YYYYMMDD'\n        set @String = @String+'000000'\n    if upper(@FormatoEntrada) = 'YYYYMMDDHH'\n        set @String = @String+'0000'\n    if upper(@FormatoEntrada) = 'DDMMYYYY'\n        set @String = right(@String,4)+substring(@String,3,2)+left(@String,2)+'000000'\n    if upper(@FormatoEntrada) = 'DDMMYYYYHHMM'\n        set @String = substring(@String,5,4)+substring(@String,3,2)+ left(@String,2)+right(@String,4)+'00'\n    if upper(@FormatoEntrada) = 'YYYYMMDDHHMM'\n        set @String = @String+'00'\n    if upper(@FormatoEntrada) = 'DDMMYYYYHHMMSS'\n        set @String = substring(@String,5,4)+substring(@String,3,2)+ left(@String,2)+right(@String,6)\n        \n    if upper(@FormatoEntrada) = 'YYYYMMDDHHMMSSMS'\n        set @String = LEFT(@String,14)\n    RETURN @String\nEND\nGO\n\n\nHere are some teste cases:\n\n--exemplos de cenÃ¡rios //{json(item()).control_column}\n-- Chegou no formado datetime 2021-07-19 15:17:30.580\ndeclare @item_controlcolumn datetime\nset @item_controlcolumn = getdate()\nselect CONVERT(BIGINT,[dbo].[REGEXP_REPLACE](CONVERT(varchar(max),@item_controlcolumn,121),'^0-9','YYYYMMDDHHMMSSMS'))\n/*\n--------------------\n20210720092826\n*/\n--chegou no formato char 'at2021-01-01time09:15:40 timezone br'\ndeclare @item_controlcolumn varchar(max)\nset @item_controlcolumn = 'at2021-01-01time09:15:40 timezone br'\nselect CONVERT(BIGINT,[dbo].[REGEXP_REPLACE](CONVERT(varchar(max),@item_controlcolumn,121),'^0-9','YYYYMMDDHHMM'))\n/*\n--------------------\n2021010109154000\n*/\n--chegou no formato char '202108'\ndeclare @item_controlcolumn varchar(max)\nset @item_controlcolumn = '202108'\nselect CONVERT(BIGINT,[dbo].[REGEXP_REPLACE](CONVERT(varchar(max),@item_controlcolumn,121),'^0-9','YYYYMM'))\n/*\n--------------------\n20210801000000\n*/\n\n--chegou no formato char '082021', invertido teria que passar esse parametro MMYYYY]\ndeclare @item_controlcolumn varchar(max)\nset @item_controlcolumn = '082021'\nselect CONVERT(BIGINT,[dbo].[REGEXP_REPLACE](CONVERT(varchar(max),@item_controlcolumn,121),'^0-9','MMYYYY'))\n/*\n--------------------\n20210801000000\n*/\n\n--chegou no formato char '15/08/2021' brasileiro, teria que passar o parametro 'DDMMYYYY'\ndeclare @item_controlcolumn varchar(max)\nset @item_controlcolumn = '15/08/2021'\nselect CONVERT(BIGINT,[dbo].[REGEXP_REPLACE](CONVERT(varchar(max),@item_controlcolumn,121),'^0-9','DDMMYYYY'))\n/*\n--------------------\n20210815000000\n*/\n--chegou no formato char '15/08/2021 19:17' brasileiro 'DDMMYYYYHHMM'\ndeclare @item_controlcolumn varchar(max)\nset @item_controlcolumn = '15/08/2021 19:17'\nselect CONVERT(BIGINT,[dbo].[REGEXP_REPLACE](CONVERT(varchar(max),@item_controlcolumn,121),'^0-9','DDMMYYYYHHMM'))\n/*\n\n",
		"activities": [
			{
				"name": "partition_eq__int",
				"description": "Filtering control column type as in or bigint, if parameter \"control_column_type_2_db\" in table dictionary is integer and sequential.",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "partition_columns_eq_control_column",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('partition_columns_eq_control_column').output.value",
						"type": "Expression"
					},
					"condition": {
						"value": "@or(equals(toLower(json(item()).control_column_type_2_db), 'int'), equals(toLower(json(item()).control_column_type_2_db), 'bigint'))",
						"type": "Expression"
					}
				}
			},
			{
				"name": "partition_eq__datetime",
				"description": "Filtering control column type as int, if parameter \"control_column_type_2_db\" in table dictionary is datetime.\n\nThis parameter \"control_column_type_2_db\" isn't the original type from database, but rather the behavior of the data. So it could be a string, int or datetime. It will work only if the column represents a datetime format.",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "partition_columns_eq_control_column",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('partition_columns_eq_control_column').output.value",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).control_column_type_2_db), 'datetime')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "partition_eq__string",
				"description": "Filtering control column type as string, if parameter \"control_column_type_2_db\" in table dictionary is string",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "partition_columns_eq_control_column",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('partition_columns_eq_control_column').output.value",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).control_column_type_2_db), 'string')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "partitioned",
				"description": "Checks if the partition column parameter is null. If not null, the load will be partitioned from this column.",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "start",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@pipeline().parameters.tables",
						"type": "Expression"
					},
					"condition": {
						"value": "@not(equals(toLower(json(item()).partition_column), 'null'))",
						"type": "Expression"
					}
				}
			},
			{
				"name": "not_partitioned",
				"description": "Checks if the partition column parameter is null. If null, table will be loaded without partitioning",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "start",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@pipeline().parameters.tables",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).partition_column), 'null')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "partition_columns_eq_control_column",
				"description": "Checks if partition column is the same as the control column defined on the parameters as 'partition_column' and 'control_column'",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "partitioned",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('partitioned').output.value",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).partition_column), toLower(json(item()).control_column))",
						"type": "Expression"
					}
				}
			},
			{
				"name": "partition_columns_noteq_control_column",
				"description": "Checks if partition column is distinct from the control column defined on the parameters as 'partition_column' and 'control_column'",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "partitioned",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('partitioned').output.value",
						"type": "Expression"
					},
					"condition": {
						"value": "@not(equals(toLower(json(item()).partition_column), toLower(json(item()).control_column)))",
						"type": "Expression"
					}
				}
			},
			{
				"name": "1_for_control_column_datetime__partitioned",
				"description": "This makes our pipeline generic and iterable. After filtering the incremental tables that have the control column that behaves like a datetime and you want to load all the data until the current date (not by year), we shall proceed working with on any of them . \n\nAfter passing through this loop, \"tables\" pipeline parameter must be casted to json when needed.\n\nIn general guidelines, control_column/partition_column must be of type INT/BIGINT so ADF can generate a valid dynamic range. Any other datatype used for this load type, you are on your own.\n\nWARNING: We tested this pipeline only for the table INDDESEMPENHO.ESTABELECIMENTO that has the control column of datetime type. It has not been already tested for a case in which the control column has a integer type that behaves like a datetime. It should work, but we don't have a test case yet.\n\n\nIMPORTANT: to use this case you need to implemenr dbo.REGEXP_REPLACE in SQLServer cause it is not native! Ask a dba for this!\n\n",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "partition_eq__datetime",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('partition_eq__datetime').output.value",
						"type": "Expression"
					},
					"isSequential": false,
					"batchCount": 4,
					"activities": [
						{
							"name": "1_run_raw_notebook",
							"description": "Invokes the pipeline for running Databricks",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "1_load_delta_incremental_table_partitioned_copy1",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__4__run_databricks_notebook",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"databricks": {
										"value": "@pipeline().parameters.databricks",
										"type": "Expression"
									},
									"adf": {
										"value": "@pipeline().parameters.adf",
										"type": "Expression"
									},
									"dls": {
										"value": "@pipeline().parameters.dls",
										"type": "Expression"
									},
									"table": {
										"value": "@string(item())",
										"type": "Expression"
									}
								}
							}
						},
						{
							"name": "1_load_delta_incremental_table_partitioned_copy1",
							"description": "Loading the incremental delta requires a lowerbound and an upperbound. \nUpperbound was generated by the lookup activity \"1_get_max_control_column_in_source_db\". And lowerbound was generated by the lookup activity \"1_get_watermark_row_for_table\".\nFor this case, the we'll partition the load by the control column.\n\nIn order to be able to receive control column as integer or datetimes, we generalized converting to timestamp, then to string and then to int.\n\nWe had so many trouble with some string columns, so we decided to use REGEX and TRIM to ensure the we trim all the characters that aren't STRICTLY NUMBERS or elements of datetime such as \":\", \"/\", \"-\" and whitespace.\n\nWARNING: All the limits must be inclusive, that is, the the control column should be => lowerbound and <= upperbound!! It's possible that we are getting redundant compared to the last load, because all the limits are inclusive. Don't worry, it will be considered in the processing pipeline with Spark.",
							"type": "Copy",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 1,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlServerSource",
									"sqlReaderQuery": {
										"value": "SELECT @{json(item()).columns} FROM @{json(item()).schema}.@{json(item()).table} WHERE ?AdfDynamicRangePartitionCondition AND COALESCE(\nCONVERT(BIGINT,@{pipeline().globalParameters.mssqlserver__function__regexp_replace}(CONVERT(varchar(max), @{json(item()).partition_column}, 121),'^0-9','@{json(item()).control_column_mask_value}')), CAST('@{pipeline().parameters.increment.control_column.lowerbound}' AS BIGINT)) >= @{pipeline().parameters.increment.control_column.lowerbound} AND COALESCE(CONVERT(BIGINT,@{pipeline().globalParameters.mssqlserver__function__regexp_replace}(CONVERT(varchar(max), @{json(item()).partition_column}, 121),'^0-9','@{json(item()).control_column_mask_value}')),\nCAST('@{pipeline().parameters.increment.control_column.lowerbound}' AS BIGINT)) <= @{pipeline().parameters.increment.control_column.upperbound}",
										"type": "Expression"
									},
									"queryTimeout": "03:20:00",
									"isolationLevel": "ReadCommitted",
									"partitionOption": "DynamicRange",
									"partitionSettings": {
										"partitionColumnName": {
											"value": "@{json(item()).partition_column}",
											"type": "Expression"
										},
										"partitionUpperBound": {
											"value": "@{pipeline().parameters.increment.control_column.upperbound}",
											"type": "Expression"
										},
										"partitionLowerBound": {
											"value": "@{pipeline().parameters.increment.control_column.lowerbound}",
											"type": "Expression"
										}
									}
								},
								"sink": {
									"type": "ParquetSink",
									"storeSettings": {
										"type": "AzureBlobFSWriteSettings"
									},
									"formatSettings": {
										"type": "ParquetWriteSettings"
									}
								},
								"enableStaging": false,
								"parallelCopies": {
									"value": "@json(item()).partitions",
									"type": "Expression"
								},
								"enableSkipIncompatibleRow": true,
								"redirectIncompatibleRowSettings": {
									"linkedServiceName": {
										"referenceName": "cnibigdatadlsgen2stg",
										"type": "LinkedServiceReference"
									},
									"path": {
										"value": "@{pipeline().parameters.container}/@{pipeline().parameters.dls.folders.error}/dbo/@{json(item()).schema}/@{json(item()).table}",
										"type": "Expression"
									}
								}
							},
							"inputs": [
								{
									"referenceName": "sqlserver_parametrized",
									"type": "DatasetReference"
								}
							],
							"outputs": [
								{
									"referenceName": "adls_parameterized_partitioned_source",
									"type": "DatasetReference",
									"parameters": {
										"container": {
											"value": "@pipeline().parameters.container",
											"type": "Expression"
										},
										"url": {
											"value": "@pipeline().parameters.url",
											"type": "Expression"
										}
									}
								}
							]
						},
						{
							"name": "1_raw_load_dbo_unified__5_update_watermark_in_db",
							"description": "Updates the watermark in watermark's database.",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "1_run_raw_notebook",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__5__update_watermark_in_db",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"watermark": {
										"value": "@pipeline().parameters.watermark",
										"type": "Expression"
									},
									"increment": {
										"value": "@pipeline().parameters.increment",
										"type": "Expression"
									},
									"table": {
										"value": "@json(item())",
										"type": "Expression"
									}
								}
							}
						}
					]
				}
			},
			{
				"name": "partition_noteq__datetime",
				"description": "Filtering control column type as int, if parameter \"control_column_type_2_db\" in table dictionary is datetime, if control column is not equal to partition column.\n\nThis parameter \"control_column_type_2_db\" isn't the original type from database, but rather the behavior of the data. So it could be a string, int or datetime. It will work only if the column represents a datetime format.",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "partition_columns_noteq_control_column",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('partition_columns_noteq_control_column').output.value",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).control_column_type_2_db), 'datetime')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "2_for_control_column_datetime__partitioned",
				"description": "This makes our pipeline generic and iterable. After filtering the incremental tables that have the control column that behaves like a datetime and you want to load all the data until the current date (not by year), we shall proceed working with on any of them . \n\nAfter passing through this loop, \"tables\" pipeline parameter must be casted to json when needed.\n\nWARNING: We tested this pipeline only for the table INDDESEMPENHO.ESTABELECIMENTO that has the control column of datetime type. It has not been already tested for a case in which the control column has a integer type that behaves like a datetime. It should work, but we don't have a test case yet.\n\nIMPORTANT: to use this case you need to implemenr dbo.REGEXP_REPLACE in SQLServer cause it is not native! Ask a dba for this!\n",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "partition_noteq__datetime",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('partition_noteq__datetime').output.value",
						"type": "Expression"
					},
					"isSequential": false,
					"batchCount": 4,
					"activities": [
						{
							"name": "2_get_max_partition_column",
							"description": "This query gets the maximum partition column value that is between the control column's limits.",
							"type": "Lookup",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 2,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlServerSource",
									"sqlReaderQuery": {
										"value": "SELECT \nMAX(@{json(item()).partition_column}) as max_partition_column \nFROM @{json(item()).schema}.@{json(item()).table}\nWHERE \nCONVERT(BIGINT,@{pipeline().globalParameters.mssqlserver__function__regexp_replace}(CONVERT(varchar(max), @{json(item()).control_column}, 121),'^0-9','@{json(item()).control_column_mask_value}'))\n>= \nCAST(@{pipeline().parameters.increment.control_column.lowerbound} AS BIGINT)\nAND \nCONVERT(BIGINT,@{pipeline().globalParameters.mssqlserver__function__regexp_replace}(CONVERT(varchar(max), @{json(item()).control_column}, 121),'^0-9','@{json(item()).control_column_mask_value}'))\n<= \nCAST(@{pipeline().parameters.increment.control_column.upperbound} AS BIGINT)",
										"type": "Expression"
									},
									"queryTimeout": "05:00:00",
									"isolationLevel": "ReadCommitted",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "sqlserver_parametrized",
									"type": "DatasetReference"
								}
							}
						},
						{
							"name": "2_get_min_partition_column",
							"description": "This query gets the minimum partition column value that is between the control column's limits.",
							"type": "Lookup",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 2,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlServerSource",
									"sqlReaderQuery": {
										"value": "SELECT \nMIN(@{json(item()).partition_column}) as min_partition_column \nFROM @{json(item()).schema}.@{json(item()).table}\nWHERE \nCONVERT(BIGINT,@{pipeline().globalParameters.mssqlserver__function__regexp_replace}(CONVERT(varchar(max), @{json(item()).control_column}, 121),'^0-9','@{json(item()).control_column_mask_value}'))\n>= \nCAST(@{pipeline().parameters.increment.control_column.lowerbound} AS BIGINT)\nAND \nCONVERT(BIGINT,@{pipeline().globalParameters.mssqlserver__function__regexp_replace}(CONVERT(varchar(max), @{json(item()).control_column}, 121),'^0-9','@{json(item()).control_column_mask_value}'))\n<= \nCAST(@{pipeline().parameters.increment.control_column.upperbound} AS BIGINT)",
										"type": "Expression"
									},
									"queryTimeout": "05:00:00",
									"isolationLevel": "ReadCommitted",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "sqlserver_parametrized",
									"type": "DatasetReference"
								}
							}
						},
						{
							"name": "check_partition_max_and_min_is_not_null",
							"description": "If max and min for partition_column are null, this menas there's no new records to load. When it happens, just wait and throw complete/success.\n\nIf they are not null, good to go and load as usual. ",
							"type": "IfCondition",
							"dependsOn": [
								{
									"activity": "2_get_max_partition_column",
									"dependencyConditions": [
										"Succeeded"
									]
								},
								{
									"activity": "2_get_min_partition_column",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"expression": {
									"value": "@and(not(equals(coalesce(activity('2_get_min_partition_column').output.firstRow.min_partition_column, 'is_null'), 'isnull')), not(equals(coalesce(activity('2_get_max_partition_column').output.firstRow.max_partition_column, 'is_null'), 'is_null')))",
									"type": "Expression"
								},
								"ifFalseActivities": [
									{
										"name": "load_success",
										"description": "No errors in \"completed\" conditions found. ",
										"type": "Wait",
										"dependsOn": [],
										"userProperties": [],
										"typeProperties": {
											"waitTimeInSeconds": 1
										}
									}
								],
								"ifTrueActivities": [
									{
										"name": "2_mssqlserver_check_partition_keep_records",
										"description": "Invokes the pipeline that checks for data loss when considering double filtering of for control_column and partition ranges.\n\ncontrol_column range is casted BIGINT.\n",
										"type": "ExecutePipeline",
										"dependsOn": [],
										"userProperties": [],
										"typeProperties": {
											"pipeline": {
												"referenceName": "mssqlserver_check_partition_keep_records",
												"type": "PipelineReference"
											},
											"waitOnCompletion": true,
											"parameters": {
												"table": {
													"value": "@json(item())",
													"type": "Expression"
												},
												"query": {
													"value": "{\"filters\":{\"control_column\":{\"lowerbound\":@{pipeline().parameters.increment.control_column.lowerbound},\"upperbound\":@{pipeline().parameters.increment.control_column.upperbound}} ,\"partition_column\":{\"lowerbound\":@{activity('2_get_min_partition_column').output.firstRow.min_partition_column},\"upperbound\":@{activity('2_get_max_partition_column').output.firstRow.max_partition_column}}}}",
													"type": "Expression"
												},
												"db": {
													"value": "@pipeline().parameters.db",
													"type": "Expression"
												},
												"adf": {
													"value": "@pipeline().parameters.adf",
													"type": "Expression"
												},
												"watermark": {
													"value": "@pipeline().parameters.watermark",
													"type": "Expression"
												}
											}
										}
									},
									{
										"name": "2_run_raw_notebook",
										"description": "Invokes the pipeline for running Databricks",
										"type": "ExecutePipeline",
										"dependsOn": [
											{
												"activity": "2_load_delta_incremental_table_partitioned",
												"dependencyConditions": [
													"Succeeded"
												]
											}
										],
										"userProperties": [],
										"typeProperties": {
											"pipeline": {
												"referenceName": "raw_load_dbo_unified__4__run_databricks_notebook",
												"type": "PipelineReference"
											},
											"waitOnCompletion": true,
											"parameters": {
												"databricks": {
													"value": "@pipeline().parameters.databricks",
													"type": "Expression"
												},
												"adf": {
													"value": "@pipeline().parameters.adf",
													"type": "Expression"
												},
												"dls": {
													"value": "@pipeline().parameters.dls",
													"type": "Expression"
												},
												"table": {
													"value": "@string(item())",
													"type": "Expression"
												}
											}
										}
									},
									{
										"name": "2_raw_load_dbo_unified__5_update_watermark_in_db",
										"description": "Updates the watermark in watermark's database.",
										"type": "ExecutePipeline",
										"dependsOn": [
											{
												"activity": "2_run_raw_notebook",
												"dependencyConditions": [
													"Succeeded"
												]
											}
										],
										"userProperties": [],
										"typeProperties": {
											"pipeline": {
												"referenceName": "raw_load_dbo_unified__5__update_watermark_in_db",
												"type": "PipelineReference"
											},
											"waitOnCompletion": true,
											"parameters": {
												"watermark": {
													"value": "@pipeline().parameters.watermark",
													"type": "Expression"
												},
												"increment": {
													"value": "@pipeline().parameters.increment",
													"type": "Expression"
												},
												"table": {
													"value": "@json(item())",
													"type": "Expression"
												}
											}
										}
									},
									{
										"name": "2_load_delta_incremental_table_partitioned",
										"description": "Loading the incremental delta requires a lowerbound and an upperbound. \nUpperbound was generated by the lookup activity \"2_get_max_control_column_in_source_db\". And lowerbound was generated by the lookup activity \"2_get_watermark_row_for_table\".\nFor this case, the we'll partition the load by the control column.\n\nIn order to be able to receive control column as integer or datetimes, we generalized converting to timestamp, then to string and then to int.\n\nWe had so many trouble with some string columns, so we decided to use REGEX and TRIM to ensure the we trim all the characters that aren't STRICTLY NUMBERS or elements of datetime such as \":\", \"/\", \"-\" and whitespace.\n\nWARNING: All the limits must be inclusive, that is, the the control column should be => lowerbound and <= upperbound!! It's possible that we are getting redundant compared to the last load, because all the limits are inclusive. Don't worry, it will be considered in the processing pipeline with Spark.",
										"type": "Copy",
										"dependsOn": [
											{
												"activity": "2_mssqlserver_check_partition_keep_records",
												"dependencyConditions": [
													"Succeeded"
												]
											}
										],
										"policy": {
											"timeout": "7.00:00:00",
											"retry": 2,
											"retryIntervalInSeconds": 30,
											"secureOutput": false,
											"secureInput": false
										},
										"userProperties": [],
										"typeProperties": {
											"source": {
												"type": "SqlServerSource",
												"sqlReaderQuery": {
													"value": "SELECT @{json(item()).columns}\nFROM @{json(item()).schema}.@{json(item()).table}\nWHERE ?AdfDynamicRangePartitionCondition AND\nCOALESCE(CONVERT(BIGINT,@{pipeline().globalParameters.mssqlserver__function__regexp_replace}(CONVERT(varchar(max), @{json(item()).control_column}, 121),'^0-9','@{json(item()).control_column_mask_value}')), CAST('@{pipeline().parameters.increment.control_column.lowerbound}' AS BIGINT))\n>= CAST('@{pipeline().parameters.increment.control_column.lowerbound}' AS BIGINT)\nAND \nCOALESCE(CONVERT(BIGINT,@{pipeline().globalParameters.mssqlserver__function__regexp_replace}(CONVERT(varchar(max), @{json(item()).control_column}, 121),'^0-9','@{json(item()).control_column_mask_value}')), CAST('@{pipeline().parameters.increment.control_column.lowerbound}' AS BIGINT))\n<= CAST(@{pipeline().parameters.increment.control_column.upperbound} AS BIGINT)\nAND \nCAST(COALESCE(@{json(item()).partition_column}, @{activity('2_get_min_partition_column').output.firstRow.min_partition_column}) AS BIGINT)\n>= @{activity('2_get_min_partition_column').output.firstRow.min_partition_column} \nAND CAST(COALESCE(@{json(item()).partition_column}, @{activity('2_get_min_partition_column').output.firstRow.min_partition_column}) AS BIGINT)\n<= @{activity('2_get_max_partition_column').output.firstRow.max_partition_column}",
													"type": "Expression"
												},
												"queryTimeout": "05:00:00",
												"isolationLevel": "ReadCommitted",
												"partitionOption": "DynamicRange",
												"partitionSettings": {
													"partitionColumnName": {
														"value": "@{json(item()).partition_column}",
														"type": "Expression"
													},
													"partitionUpperBound": {
														"value": "@{activity('2_get_max_partition_column').output.firstRow.max_partition_column}",
														"type": "Expression"
													},
													"partitionLowerBound": {
														"value": "@{activity('2_get_min_partition_column').output.firstRow.min_partition_column}",
														"type": "Expression"
													}
												}
											},
											"sink": {
												"type": "ParquetSink",
												"storeSettings": {
													"type": "AzureBlobFSWriteSettings"
												},
												"formatSettings": {
													"type": "ParquetWriteSettings"
												}
											},
											"enableStaging": false,
											"parallelCopies": {
												"value": "@json(item()).partitions",
												"type": "Expression"
											},
											"enableSkipIncompatibleRow": false,
											"validateDataConsistency": true,
											"translator": {
												"type": "TabularTranslator",
												"typeConversion": true,
												"typeConversionSettings": {
													"allowDataTruncation": true,
													"treatBooleanAsNumber": false
												}
											}
										},
										"inputs": [
											{
												"referenceName": "sqlserver_parametrized",
												"type": "DatasetReference"
											}
										],
										"outputs": [
											{
												"referenceName": "adls_parameterized_partitioned_source",
												"type": "DatasetReference",
												"parameters": {
													"container": {
														"value": "@pipeline().parameters.container",
														"type": "Expression"
													},
													"url": {
														"value": "@pipeline().parameters.url",
														"type": "Expression"
													}
												}
											}
										]
									}
								]
							}
						}
					]
				}
			},
			{
				"name": "datetime",
				"description": "Filtering control column type as int, if parameter \"control_column_type_2_db\" in table dictionary is datetime.\n\nThis parameter \"control_column_type_2_db\" isn't the original type from database, but rather the behavior of the data. So it could be a string, int or datetime. It will work only if the column represents a datetime format.",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "not_partitioned",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('not_partitioned').output.value\n",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).control_column_type_2_db), 'datetime')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "3_for__control_column_datetime",
				"description": "This makes our pipeline generic and iterable. After filtering the incremental tables that have the control column that behaves like a datetime and you want to load all the data until the current datetime (not by year), we shall proceed working with on any of them . \n\nAfter passing through this loop, \"tables\" pipeline parameter must be casted to json when needed.\n\nWARNING: We tested this pipeline only for the table INDDESEMPENHO.ESTABELECIMENTO that has the control column of datetime type. It has not been already tested for a case in which the control column has a integer type that behaves like a datetime. It should work, but we don't have a test case yet.\n\n",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "datetime",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('datetime').output.value\n",
						"type": "Expression"
					},
					"isSequential": false,
					"batchCount": 4,
					"activities": [
						{
							"name": "3_load_delta_incremental_table",
							"description": "Loading the incremental delta requires a lowerbound and an upperbound. \nUpperbound was generated by the lookup activity \"1_get_max_control_column_in_source_db\". And lowerbound was generated by the lookup activity \"1_get_watermark_row_for_table\".\n\nIn order to be able to receive control column as integer or datetimes, we generalized converting to timestamp, then to string and then to int.\n\nWe had so many trouble with some string columns, so we decided to use REGEX and TRIM to ensure the we trim all the characters that aren't STRICTLY NUMBERS or elements of datetime such as \":\", \"/\", \"-\" and whitespace.\n\nWARNING: All the limits must be inclusive, that is, the the control column should be => lowerbound and <= upperbound!! It's possible that we are getting redundant compared to the last load, because all the limits are inclusive. Don't worry, it will be considered in the processing pipeline with Spark.",
							"type": "Copy",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 2,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlServerSource",
									"sqlReaderQuery": {
										"value": "SELECT @{json(item()).columns} FROM @{json(item()).schema}.@{json(item()).table} WHERE CAST(COALESCE(FORMAT(@{json(item()).control_column}, 'yyyyMMddhhmmss'), '@{pipeline().parameters.increment.control_column.lowerbound}')\nAS BIGINT) \n>= CAST('@{pipeline().parameters.increment.control_column.lowerbound}' AS BIGINT) \nAND \nCAST(COALESCE(FORMAT(@{json(item()).control_column}, 'yyyyMMddhhmmss'), '@{pipeline().parameters.increment.control_column.lowerbound}')\nAS BIGINT)\n<= CAST('@{pipeline().parameters.increment.control_column.upperbound}' AS BIGINT)",
										"type": "Expression"
									},
									"queryTimeout": "02:00:00",
									"partitionOption": "None"
								},
								"sink": {
									"type": "ParquetSink",
									"storeSettings": {
										"type": "AzureBlobFSWriteSettings"
									},
									"formatSettings": {
										"type": "ParquetWriteSettings"
									}
								},
								"enableStaging": false,
								"enableSkipIncompatibleRow": true,
								"redirectIncompatibleRowSettings": {
									"linkedServiceName": {
										"referenceName": "cnibigdatadlsgen2stg",
										"type": "LinkedServiceReference"
									},
									"path": {
										"value": "@{pipeline().parameters.container}/@{pipeline().parameters.dls.folders.error}/dbo/@{json(item()).schema}/@{json(item()).table}",
										"type": "Expression"
									}
								}
							},
							"inputs": [
								{
									"referenceName": "sqlserver_parametrized",
									"type": "DatasetReference"
								}
							],
							"outputs": [
								{
									"referenceName": "adls_parameterized_partitioned_source",
									"type": "DatasetReference",
									"parameters": {
										"container": {
											"value": "@pipeline().parameters.container",
											"type": "Expression"
										},
										"url": {
											"value": "@pipeline().parameters.url",
											"type": "Expression"
										}
									}
								}
							]
						},
						{
							"name": "3_run_raw_notebook",
							"description": "Invokes the pipeline for running Databricks",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "3_load_delta_incremental_table",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__4__run_databricks_notebook",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"databricks": {
										"value": "@pipeline().parameters.databricks",
										"type": "Expression"
									},
									"adf": {
										"value": "@pipeline().parameters.adf",
										"type": "Expression"
									},
									"dls": {
										"value": "@pipeline().parameters.dls",
										"type": "Expression"
									},
									"table": {
										"value": "@string(item())",
										"type": "Expression"
									}
								}
							}
						},
						{
							"name": "3_raw_load_dbo_unified__5_update_watermark_in_db",
							"description": "Updates the watermark in watermark's database.",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "3_run_raw_notebook",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__5__update_watermark_in_db",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"watermark": {
										"value": "@pipeline().parameters.watermark",
										"type": "Expression"
									},
									"increment": {
										"value": "@pipeline().parameters.increment",
										"type": "Expression"
									},
									"table": {
										"value": "@json(item())",
										"type": "Expression"
									}
								}
							}
						}
					]
				}
			},
			{
				"name": "int",
				"description": "Filtering control column type as int, if parameter \"control_column_type_2_db\" in table dictionary is int/bigint.",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "not_partitioned",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('not_partitioned').output.value\n",
						"type": "Expression"
					},
					"condition": {
						"value": "@or(equals(toLower(json(item()).control_column_type_2_db), 'int'), equals(toLower(json(item()).control_column_type_2_db), 'bigint'))",
						"type": "Expression"
					}
				}
			},
			{
				"name": "4_for_control_column_int",
				"description": "This makes our pipeline generic and iterable. After filtering the incremental tables that have the control column that behaves like a datetime and you want to load all the data until the current datetime (not by year), we shall proceed working with on any of them . \n\nAfter passing through this loop, \"tables\" pipeline parameter must be casted to json when needed.\n",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "int",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('int').output.value\n",
						"type": "Expression"
					},
					"isSequential": false,
					"batchCount": 4,
					"activities": [
						{
							"name": "4_load_delta_incremental_copy",
							"description": "Loading the incremental delta requires a lowerbound and an upperbound. \nUpperbound was generated by the lookup activity \"1_get_max_control_column_in_source_db\". And lowerbound was generated by the lookup activity \"1_get_watermark_row_for_table\".\n\nIn order to be able to receive control column as integer or datetimes, we generalized converting to timestamp, then to string and then to int.\n\nWe had so many trouble with some string columns, so we decided to use REGEX and TRIM to ensure the we trim all the characters that aren't STRICTLY NUMBERS or elements of datetime such as \":\", \"/\", \"-\" and whitespace.\n\nWARNING: All the limits must be inclusive, that is, the the control column should be => lowerbound and <= upperbound!! It's possible that we are getting redundant compared to the last load, because all the limits are inclusive. Don't worry, it will be considered in the processing pipeline with Spark.",
							"type": "Copy",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 2,
								"retryIntervalInSeconds": 60,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlServerSource",
									"sqlReaderQuery": {
										"value": "SELECT @{json(item()).columns}\nFROM @{json(item()).schema}.@{json(item()).table}\nWHERE\n@{json(item()).control_column} > @{pipeline().parameters.increment.control_column.lowerbound}\nAND \n@{json(item()).control_column} <= COALESCE(@{pipeline().parameters.increment.control_column.upperbound},\n@{pipeline().parameters.increment.control_column.lowerbound})",
										"type": "Expression"
									},
									"queryTimeout": "05:00:00",
									"isolationLevel": "ReadCommitted",
									"partitionOption": "None"
								},
								"sink": {
									"type": "ParquetSink",
									"storeSettings": {
										"type": "AzureBlobFSWriteSettings"
									},
									"formatSettings": {
										"type": "ParquetWriteSettings"
									}
								},
								"enableStaging": false,
								"enableSkipIncompatibleRow": true,
								"redirectIncompatibleRowSettings": {
									"linkedServiceName": {
										"referenceName": "cnibigdatadlsgen2stg",
										"type": "LinkedServiceReference"
									},
									"path": {
										"value": "@{pipeline().parameters.container}/@{pipeline().parameters.dls.folders.error}/dbo/@{json(item()).schema}/@{json(item()).table}",
										"type": "Expression"
									}
								}
							},
							"inputs": [
								{
									"referenceName": "sqlserver_parametrized",
									"type": "DatasetReference"
								}
							],
							"outputs": [
								{
									"referenceName": "adls_parameterized_partitioned_source",
									"type": "DatasetReference",
									"parameters": {
										"container": {
											"value": "@pipeline().parameters.container",
											"type": "Expression"
										},
										"url": {
											"value": "@pipeline().parameters.url",
											"type": "Expression"
										}
									}
								}
							]
						},
						{
							"name": "4_run_raw_notebook",
							"description": "Invokes the pipeline for running Databricks",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "4_load_delta_incremental_copy",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__4__run_databricks_notebook",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"databricks": {
										"value": "@pipeline().parameters.databricks",
										"type": "Expression"
									},
									"adf": {
										"value": "@pipeline().parameters.adf",
										"type": "Expression"
									},
									"dls": {
										"value": "@pipeline().parameters.dls",
										"type": "Expression"
									},
									"table": {
										"value": "@string(item())",
										"type": "Expression"
									}
								}
							}
						},
						{
							"name": "4_raw_load_dbo_unified__5_update_watermark_in_db",
							"description": "Updates the watermark in watermark's database.",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "4_run_raw_notebook",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__5__update_watermark_in_db",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"watermark": {
										"value": "@pipeline().parameters.watermark",
										"type": "Expression"
									},
									"increment": {
										"value": "@pipeline().parameters.increment",
										"type": "Expression"
									},
									"table": {
										"value": "@json(item())",
										"type": "Expression"
									}
								}
							}
						}
					]
				}
			},
			{
				"name": "start",
				"description": "Wait activity just to make it beautiful. =D",
				"type": "Wait",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"waitTimeInSeconds": 1
				}
			},
			{
				"name": "0_for_control_column_int__partitioned",
				"description": "This makes our pipeline generic and iterable. After filtering the incremental tables that have the control column that behaves like a datetime and you want to load all the data until the current date (not by year), we shall proceed working with on any of them . \n\nAfter passing through this loop, \"tables\" pipeline parameter must be casted to json when needed.\n\n",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "partition_eq__int",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('partition_eq__int').output.value",
						"type": "Expression"
					},
					"isSequential": false,
					"batchCount": 4,
					"activities": [
						{
							"name": "0_load_delta_incremental_table_partitioned",
							"description": "Loading the incremental delta requires a lowerbound and an upperbound. \nUpperbound was generated by the lookup activity \"1_get_max_control_column_in_source_db\". And lowerbound was generated by the lookup activity \"1_get_watermark_row_for_table\".\nFor this case, the we'll partition the load by the control column.\n\nIn order to be able to receive control column as integer or datetimes, we generalized converting to timestamp, then to string and then to int.\n\nWe had so many trouble with some string columns, so we decided to use REGEX and TRIM to ensure the we trim all the characters that aren't STRICTLY NUMBERS or elements of datetime such as \":\", \"/\", \"-\" and whitespace.\n\nWARNING: All the limits must be inclusive, that is, the the control column should be => lowerbound and <= upperbound!! It's possible that we are getting redundant compared to the last load, because all the limits are inclusive. Don't worry, it will be considered in the processing pipeline with Spark.\n\nRemember to change the sink dataset to \"adls_parametrized_partitioned_source_v2\" when databricks implementation is finished!",
							"type": "Copy",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 1,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "AzureSqlSource",
									"sqlReaderQuery": {
										"value": "SELECT @{json(item()).columns} FROM @{json(item()).schema}.@{json(item()).table} WHERE\n?AdfDynamicRangePartitionCondition AND CAST(@{json(item()).control_column} AS BIGINT) >= @{pipeline().parameters.increment.control_column.lowerbound} AND CAST(@{json(item()).control_column} AS BIGINT) <= @{pipeline().parameters.increment.control_column.upperbound}",
										"type": "Expression"
									},
									"queryTimeout": "02:00:00",
									"partitionOption": "DynamicRange",
									"partitionSettings": {
										"partitionColumnName": {
											"value": "@{json(item()).partition_column}",
											"type": "Expression"
										},
										"partitionUpperBound": {
											"value": "@{pipeline().parameters.increment.control_column.upperbound}",
											"type": "Expression"
										},
										"partitionLowerBound": {
											"value": "@{pipeline().parameters.increment.control_column.lowerbound}",
											"type": "Expression"
										}
									}
								},
								"sink": {
									"type": "ParquetSink",
									"storeSettings": {
										"type": "AzureBlobFSWriteSettings"
									},
									"formatSettings": {
										"type": "ParquetWriteSettings"
									}
								},
								"enableStaging": false,
								"parallelCopies": {
									"value": "@json(item()).partitions",
									"type": "Expression"
								},
								"enableSkipIncompatibleRow": true,
								"redirectIncompatibleRowSettings": {
									"linkedServiceName": {
										"referenceName": "cnibigdatadlsgen2stg",
										"type": "LinkedServiceReference"
									},
									"path": {
										"value": "@{pipeline().parameters.container}/@{pipeline().parameters.dls.folders.error}/dbo/@{json(item()).schema}/@{json(item()).table}",
										"type": "Expression"
									}
								}
							},
							"inputs": [
								{
									"referenceName": "azuresql_parameterized",
									"type": "DatasetReference"
								}
							],
							"outputs": [
								{
									"referenceName": "adls_parameterized_partitioned_source",
									"type": "DatasetReference",
									"parameters": {
										"container": {
											"value": "@pipeline().parameters.container",
											"type": "Expression"
										},
										"url": {
											"value": "@pipeline().parameters.url",
											"type": "Expression"
										}
									}
								}
							]
						},
						{
							"name": "0_run_raw_notebook",
							"description": "Invokes the pipeline for running Databricks",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "0_load_delta_incremental_table_partitioned",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__4__run_databricks_notebook",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"databricks": {
										"value": "@pipeline().parameters.databricks",
										"type": "Expression"
									},
									"adf": {
										"value": "@pipeline().parameters.adf",
										"type": "Expression"
									},
									"dls": {
										"value": "@pipeline().parameters.dls",
										"type": "Expression"
									},
									"table": {
										"value": "@string(item())",
										"type": "Expression"
									}
								}
							}
						},
						{
							"name": "0_raw_load_dbo_unified__5_update_watermark_in_db",
							"description": "Updates the watermark in watermark's database.",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "0_run_raw_notebook",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__5__update_watermark_in_db",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"watermark": {
										"value": "@pipeline().parameters.watermark",
										"type": "Expression"
									},
									"increment": {
										"value": "@pipeline().parameters.increment",
										"type": "Expression"
									},
									"table": {
										"value": "@json(item())",
										"type": "Expression"
									}
								}
							}
						}
					]
				}
			}
		],
		"parameters": {
			"db": {
				"type": "object"
			},
			"tables": {
				"type": "array"
			},
			"dls": {
				"type": "object"
			},
			"watermark": {
				"type": "object"
			},
			"databricks": {
				"type": "object"
			},
			"adf": {
				"type": "object"
			},
			"increment": {
				"type": "object"
			},
			"container": {
				"type": "string"
			},
			"url": {
				"type": "string"
			}
		},
		"folder": {
			"name": "templates/raw/bdo/mssqlserver"
		},
		"annotations": [
			"raw"
		]
	},
	"type": "Microsoft.DataFactory/factories/pipelines"
}