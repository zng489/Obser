{
	"name": "oracle_import_full",
	"properties": {
		"description": "Well, this one uses a Filter condition for making thing work for full loads.\n\nWARNING: the tables object must be type ARRAY.\nAll objects in this array must be, in reality STRING type, enclosed by \".\nInsisde this objects, you should enclose everything in SINGLE QUOTES.\nOtherwise, things are not going to work. I warned you!\n\nHere's an example:\n[\"{'schema': 'PROTHEUS11', 'table':'AKF010','load_type':'full','partition_column':'R_E_C_N_O_','partitions':5}\"]",
		"activities": [
			{
				"name": "filter_tables_load_type_is_full",
				"description": "Using filters and a definition of the table like an object, we must be able to get things going on a flux which separates incremental and full load. ",
				"type": "Filter",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@pipeline().parameters.tables",
						"type": "Expression"
					},
					"condition": {
						"value": "@or(equals(toLower(json(item()).load_type), 'full'), equals(toLower(json(item()).load_type), 'full_balance'))",
						"type": "Expression"
					}
				}
			},
			{
				"name": "1_for_each_table_load_type_full_partitioned",
				"description": "This makes our pipeline generic and iterable. After filtering the full tables, we shall proceed working with on any of them . \n\nAfter passing through this loop, \"tables\" pipeline parameter must be casted to json when needed.\n\nAnd don't worry; if output is null, the rest of the flow is not executed! At least this!",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "filter_if_partitioned_full",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_if_partitioned_full').output.value",
						"type": "Expression"
					},
					"batchCount": 4,
					"activities": [
						{
							"name": "1_get_max_min_partition_column_in_source_db",
							"description": "Looks for the partition column in the source db. Retrieves max(partition_column) and min(partition_column). This will be used as limits for  partition definition on load.",
							"type": "Lookup",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 1,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "OracleSource",
									"oracleReaderQuery": {
										"value": "SELECT MAX(CAST(@{json(item()).partition_column} AS INT)) AS max_partition_column, MIN(CAST(@{json(item()).partition_column} AS INT)) AS min_partition_column FROM  @{json(item()).schema}.@{json(item()).table}",
										"type": "Expression"
									},
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "oracle_table_parameterized",
									"type": "DatasetReference"
								}
							}
						},
						{
							"name": "1_load_full_table_partitioned",
							"description": "Loading the full table partitioned requires a lowerbound and an upperbound. \nUpperbound was generated by the lookup activity \"1_get_max_control_column_in_source_db\". And lowerbound was generated by the lookup activity \"1_get_watermark_row_for_table\".\nFor this case, the we'll partition the load by the control column.\n\nWARNING: All the limits must be inclusive, that is, the the control column should be => lowerbound and <= upperbound!! It's possible that we are getting redundant compared to the last load, because all the limits are inclusive. Don't worry, it will be considered in the processing pipeline with Spark.\n\nRemember to change the sink dataset to \"adls_parametrized_partitioned_source_v2\" when databricks implementation is finished! ",
							"type": "Copy",
							"dependsOn": [
								{
									"activity": "1_get_max_min_partition_column_in_source_db",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 2,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "OracleSource",
									"oracleReaderQuery": {
										"value": "SELECT @{json(item()).columns} FROM @{json(item()).schema}.@{json(item()).table} WHERE CAST(NVL(?AdfRangePartitionColumnName, @{activity('1_get_max_min_partition_column_in_source_db').output.firstRow.min_partition_column}) AS INT) <= CAST(?AdfRangePartitionUpbound AS INT) AND CAST(NVL( ?AdfRangePartitionColumnName, @{activity('1_get_max_min_partition_column_in_source_db').output.firstRow.min_partition_column}) AS INT) >= CAST(?AdfRangePartitionLowbound AS INT)",
										"type": "Expression"
									},
									"partitionOption": "DynamicRange",
									"partitionSettings": {
										"partitionColumnName": {
											"value": "@{json(item()).partition_column}",
											"type": "Expression"
										},
										"partitionUpperBound": {
											"value": "@{activity('1_get_max_min_partition_column_in_source_db').output.firstRow.max_partition_column}",
											"type": "Expression"
										},
										"partitionLowerBound": {
											"value": "@{activity('1_get_max_min_partition_column_in_source_db').output.firstRow.min_partition_column}",
											"type": "Expression"
										}
									}
								},
								"sink": {
									"type": "ParquetSink",
									"storeSettings": {
										"type": "AzureBlobFSWriteSettings"
									},
									"formatSettings": {
										"type": "ParquetWriteSettings"
									}
								},
								"enableStaging": false,
								"parallelCopies": {
									"value": "@json(item()).partitions",
									"type": "Expression"
								},
								"enableSkipIncompatibleRow": true,
								"redirectIncompatibleRowSettings": {
									"linkedServiceName": {
										"referenceName": "cnibigdatadlsgen2stg",
										"type": "LinkedServiceReference"
									},
									"path": {
										"value": "@{pipeline().parameters.container}/@{pipeline().parameters.dls.folders.error}/dbo/@{json(item()).schema}/@{json(item()).table}",
										"type": "Expression"
									}
								}
							},
							"inputs": [
								{
									"referenceName": "oracle_table_parameterized",
									"type": "DatasetReference"
								}
							],
							"outputs": [
								{
									"referenceName": "adls_parameterized_partitioned_source",
									"type": "DatasetReference",
									"parameters": {
										"container": {
											"value": "@pipeline().parameters.container",
											"type": "Expression"
										},
										"url": {
											"value": "@pipeline().parameters.url",
											"type": "Expression"
										}
									}
								}
							]
						},
						{
							"name": "1_run_raw_notebook",
							"description": "Invokes the pipeline for running Databricks",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "1_load_full_table_partitioned",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__4__run_databricks_notebook",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"databricks": {
										"value": "@pipeline().parameters.databricks",
										"type": "Expression"
									},
									"adf": {
										"value": "@pipeline().parameters.adf",
										"type": "Expression"
									},
									"dls": {
										"value": "@pipeline().parameters.dls",
										"type": "Expression"
									},
									"table": {
										"value": "@string(item())",
										"type": "Expression"
									}
								}
							}
						}
					]
				}
			},
			{
				"name": "filter_if_partitioned_full",
				"description": "Checks if the partition column parameter is null. If not null, the load will be partitioned from this column.",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "filter_tables_load_type_is_full",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_tables_load_type_is_full').output.value\n",
						"type": "Expression"
					},
					"condition": {
						"value": "@not(equals(toLower(json(item()).partition_column), 'null'))",
						"type": "Expression"
					}
				}
			},
			{
				"name": "filter_if_not_partitioned_full",
				"description": "Checks if the partition column parameter is null. If null, table will be loaded without partitioning",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "filter_tables_load_type_is_full",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_tables_load_type_is_full').output.value\n",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).partition_column), 'null')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "2_for_each_table_load_type_full",
				"description": "This makes our pipeline generic and iterable. After filtering the full tables, we shall proceed working with on any of them . \n\nAfter passing through this loop, \"tables\" pipeline parameter must be casted to json when needed.\n\nAnd don't worry; if output is null, the rest of the flow is not executed! At least this!",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "filter_if_not_partitioned_full",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_if_not_partitioned_full').output.value",
						"type": "Expression"
					},
					"batchCount": 4,
					"activities": [
						{
							"name": "2_load_full_table",
							"description": "To load the table without partitioning, just execute a simple select * from table. No need to calculate upper bound, so we don't need the activity \"get_max_partition_column_in_source_db\". ",
							"type": "Copy",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 2,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "OracleSource",
									"oracleReaderQuery": {
										"value": "SELECT @{json(item()).columns} FROM @{json(item()).schema}.@{json(item()).table}",
										"type": "Expression"
									},
									"partitionOption": "None"
								},
								"sink": {
									"type": "ParquetSink",
									"storeSettings": {
										"type": "AzureBlobFSWriteSettings"
									},
									"formatSettings": {
										"type": "ParquetWriteSettings"
									}
								},
								"enableStaging": false,
								"parallelCopies": 4,
								"enableSkipIncompatibleRow": true,
								"redirectIncompatibleRowSettings": {
									"linkedServiceName": {
										"referenceName": "cnibigdatadlsgen2stg",
										"type": "LinkedServiceReference"
									},
									"path": {
										"value": "@{pipeline().parameters.container}/@{pipeline().parameters.dls.folders.error}/dbo/@{json(item()).schema}/@{json(item()).table}",
										"type": "Expression"
									}
								},
								"dataIntegrationUnits": 2
							},
							"inputs": [
								{
									"referenceName": "oracle_table_parameterized",
									"type": "DatasetReference"
								}
							],
							"outputs": [
								{
									"referenceName": "adls_parameterized_partitioned_source",
									"type": "DatasetReference",
									"parameters": {
										"container": {
											"value": "@pipeline().parameters.container",
											"type": "Expression"
										},
										"url": {
											"value": "@pipeline().parameters.url",
											"type": "Expression"
										}
									}
								}
							]
						},
						{
							"name": "2_run_raw_notebook",
							"description": "Invokes the pipeline for running Databricks",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "2_load_full_table",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__4__run_databricks_notebook",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"databricks": {
										"value": "@pipeline().parameters.databricks",
										"type": "Expression"
									},
									"adf": {
										"value": "@pipeline().parameters.adf",
										"type": "Expression"
									},
									"dls": {
										"value": "@pipeline().parameters.dls",
										"type": "Expression"
									},
									"table": {
										"value": "@string(item())",
										"type": "Expression"
									}
								}
							}
						}
					]
				}
			}
		],
		"parameters": {
			"db": {
				"type": "object"
			},
			"tables": {
				"type": "array"
			},
			"dls": {
				"type": "object"
			},
			"watermark": {
				"type": "object"
			},
			"databricks": {
				"type": "object"
			},
			"adf": {
				"type": "string"
			},
			"container": {
				"type": "string"
			},
			"url": {
				"type": "string"
			}
		},
		"folder": {
			"name": "templates/raw/bdo/oracle"
		},
		"annotations": []
	},
	"type": "Microsoft.DataFactory/factories/pipelines"
}