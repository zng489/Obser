{
	"name": "postgres_import_incremental__0__evaluator",
	"properties": {
		"description": "Well, this one uses a Filter condition for making things work for incremental loads.\n\nWARNING: \"tables\" object must be type ARRAY.\nAll objects in this array must be, in reality STRING type, enclosed by \".\nInside this objects, you should enclose everything in SINGLE QUOTES.\nOtherwise, things are not going to work. I warned you!\n\nHere's an example:\n[\"{'schema': 'INDDESEMPENHO', 'table':'ESTABELECIMENTO','load_type':'incremental','partition_column':'DATAATUALIZACAO','partitions':5,'control_column':'DATAATUALIZACAO','control_column_type_2_db':'datetime', 'control_column_default_value': '19000101',\n'control_column_mask_value': 'DD/MM/YYYY HH24:MI:SS'}\"]\n\n",
		"activities": [
			{
				"name": "get_max_datetime_control_column_in_postgres",
				"description": "Queries a predefined Postgres Instance for current_timesptam and casts as the needed string format so we can use as upperbound value for incremental loads.\n\nOBS: This is run against an specific instance, since Postgres cannot be parameterized. Your best hope is to have the clocks close in time to one another.",
				"type": "Lookup",
				"dependsOn": [],
				"policy": {
					"timeout": "7.00:00:00",
					"retry": 2,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "PostgreSqlSource",
						"query": "SELECT CAST(TO_CHAR(CURRENT_TIMESTAMP, 'YYYYMMDDHHMMSS') AS BIGINT) AS UPPERBOUND"
					},
					"dataset": {
						"referenceName": "postgres_tableau",
						"type": "DatasetReference"
					}
				}
			},
			{
				"name": "create_watermark_row_for_table_if_not_exists",
				"description": "Calls the procedure that creates a new entry in the watermark control table in SQLDW case the table is not already created in it. \nDon't worry, there's a clause in the procedure that guarantees that only non-existing tables will be created. There's no possibility of overwrite. So keep this component as it is. It looks strange but works!\n\nThe first value of the control column in the watermark table is set in the properties of each table by the variable 'control_column_default_value'.\n\nThis component can create watermarks for any data type: datetime, int, string, you choose. They all come from the 'tables' dict in the configuration.",
				"type": "SqlServerStoredProcedure",
				"dependsOn": [],
				"policy": {
					"timeout": "0.00:06:00",
					"retry": 2,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"storedProcedureName": {
						"value": "@pipeline().parameters.watermark.procedures.insert_first_watermark",
						"type": "Expression"
					},
					"storedProcedureParameters": {
						"ControlColumn": {
							"value": {
								"value": "@toLower(json(pipeline().parameters.tables[0]).control_column)",
								"type": "Expression"
							},
							"type": "String"
						},
						"ControlColumnType2Db": {
							"value": {
								"value": "@toLower(json(pipeline().parameters.tables[0]).control_column_type_2_db)",
								"type": "Expression"
							},
							"type": "String"
						},
						"ControlColumnValue": {
							"value": {
								"value": "@toLower(json(pipeline().parameters.tables[0]).control_column_default_value)",
								"type": "Expression"
							},
							"type": "String"
						},
						"TableDatabaseVendor": {
							"value": {
								"value": "@toLower(pipeline().parameters.db.vendor)",
								"type": "Expression"
							},
							"type": "String"
						},
						"TableName": {
							"value": {
								"value": "dbo.@{toLower(json(pipeline().parameters.tables[0]).schema)}.@{toLower(json(pipeline().parameters.tables[0]).table)}",
								"type": "Expression"
							},
							"type": "String"
						}
					}
				},
				"linkedServiceName": {
					"referenceName": "cnibigdatasqldw",
					"type": "LinkedServiceReference"
				}
			},
			{
				"name": "get_watermark_row_for_table",
				"description": "This lookup goes in the watermark table and fetches the entire row. \nWith this activity, we're setting the lower bound for the incremental load process. Now we need to check for datetime typed columns, so we'll be able to avoid values for a non-existing future.\n\nAfter fetching this row, we'll check for it's consistency and values. ",
				"type": "Lookup",
				"dependsOn": [
					{
						"activity": "create_watermark_row_for_table_if_not_exists",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "7.00:00:00",
					"retry": 2,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "SqlDWSource",
						"sqlReaderQuery": {
							"value": "SELECT  * FROM @{toLower(pipeline().parameters.watermark.table)} WHERE @{toLower(pipeline().parameters.watermark.columns.table_name)}  = @{toLower(concat('''dbo.', json(pipeline().parameters.tables[0]).schema, '.', json(pipeline().parameters.tables[0]).table, ''''))}",
							"type": "Expression"
						},
						"partitionOption": "None"
					},
					"dataset": {
						"referenceName": "sqldw_parameterized_watermark_table",
						"type": "DatasetReference"
					}
				}
			},
			{
				"name": "postgres_import_incremental__1__wrapper",
				"description": "Calls the next step, which implements incremental load from Postgres over valid control_column range",
				"type": "ExecutePipeline",
				"dependsOn": [
					{
						"activity": "get_watermark_row_for_table",
						"dependencyConditions": [
							"Succeeded"
						]
					},
					{
						"activity": "get_max_datetime_control_column_in_postgres",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"pipeline": {
						"referenceName": "postgres_import_incremental__1__wrapper",
						"type": "PipelineReference"
					},
					"waitOnCompletion": true,
					"parameters": {
						"db": {
							"value": "@pipeline().parameters.db",
							"type": "Expression"
						},
						"tables": {
							"value": "@pipeline().parameters.tables",
							"type": "Expression"
						},
						"dls": {
							"value": "@pipeline().parameters.dls",
							"type": "Expression"
						},
						"watermark": {
							"value": "@pipeline().parameters.watermark",
							"type": "Expression"
						},
						"databricks": {
							"value": "@pipeline().parameters.databricks",
							"type": "Expression"
						},
						"adf": {
							"value": "@pipeline().parameters.adf",
							"type": "Expression"
						},
						"increment": {
							"value": "@json(concat('{\"control_column\": {\"lowerbound\":' , string(activity('get_watermark_row_for_table').output.firstRow.control_column_value), ', \"upperbound\":', string(activity('get_max_datetime_control_column_in_postgres').output.firstRow.UPPERBOUND), '}}'))",
							"type": "Expression"
						},
						"container": {
							"value": "@pipeline().parameters.container",
							"type": "Expression"
						},
						"url": {
							"value": "@pipeline().parameters.url",
							"type": "Expression"
						}
					}
				}
			}
		],
		"parameters": {
			"db": {
				"type": "object"
			},
			"tables": {
				"type": "array"
			},
			"dls": {
				"type": "object"
			},
			"watermark": {
				"type": "object"
			},
			"databricks": {
				"type": "object"
			},
			"adf": {
				"type": "object"
			},
			"container": {
				"type": "string"
			},
			"url": {
				"type": "string"
			}
		},
		"folder": {
			"name": "templates/raw/bdo/postgres/import_incremental"
		},
		"annotations": [
			"template",
			"raw",
			"postgres"
		]
	},
	"type": "Microsoft.DataFactory/factories/pipelines"
}