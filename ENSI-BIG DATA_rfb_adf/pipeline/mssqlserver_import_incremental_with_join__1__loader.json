{
	"name": "mssqlserver_import_incremental_with_join__1__loader",
	"properties": {
		"description": "This type of load gets a control column from another table and use it to perform incremental loads.\n\nLIMITATIONS: we'll consider only 1 (and no more than one) join between tables, with only 1 column in common that will be the control column of the load.\n\nATTENTION: when specifying the columns in the parameters, state just the ones that are NOT the control column neither the join column. Those columns will be automatically added on the load by the template.\n\nWARNING: \"tables\" object must be type ARRAY.\nAll objects in this array must be, in reality STRING type, enclosed by \".\nInside this objects, you should enclose everything in SINGLE QUOTES.\nOtherwise, things are not going to work. I warned you!\n\nHere's an example:\n[\"{'schema': 'OBA', 'table':'RL_CARGO_ESCOLARID_UNID_OBA', 'control_table':'UNIDADE_OBA', 'join_type':'INNER', 'join_column':'COD_OBA','load_type':'incremental_with_join','partition_column':'null','control_column':'DT_ATUALIZACAO','control_column_type_2_db':'datetime', 'control_column_default_value': '20150520000000', 'control_column_mask_value': 'DD/MM/YYYYHH24:MI:SSFF', 'columns': 'ID_TIPO_CARGO_OBA,ID_TIPO_ESCOLARIDADE,QT_CARGO_ESCOLARIDADE,ID_AREA_ATUACAO_RH,ID_DETALHAMENTO_NEGOCIO,ID_CARGO_ESCOLARIDADE_UNID_OBA,IND_EXCLUSAO'}\"]\n\n",
		"activities": [
			{
				"name": "filter_tables_load_type_incremental_with_join",
				"description": "Using filters and a definition of the table like an object, we must be able to get things going on a flux which separates the types of load.\nIn this case, we will allow only tables that has incremental_with_join type.",
				"type": "Filter",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@pipeline().parameters.tables",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).load_type), 'incremental_with_join')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "filter_if_not_partitioned_incremental",
				"description": "Checks if the partition column parameter is null. If null, table will be loaded without partitioning",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "filter_tables_load_type_incremental_with_join",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_tables_load_type_incremental_with_join').output.value\n",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).partition_column), 'null')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "filter_control_column_type_2_db_datetime_notpart",
				"description": "Filtering control column type as int, if parameter \"control_column_type_2_db\" in table dictionary is datetime.\n\nThis parameter \"control_column_type_2_db\" isn't the original type from database, but rather the behavior of the data. So it could be a string, int or datetime. It will work only if the column represents a datetime format.",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "filter_if_not_partitioned_incremental",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_if_not_partitioned_incremental').output.value\n",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).control_column_type_2_db), 'datetime')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "1_for_each_incremental_control_column_datetime",
				"description": "This makes our pipeline generic and iterable. After filtering the incremental tables that have the control column that behaves like a datetime and you want to load all the data until the current date (not by year), we shall proceed working with on any of them . \n\nAfter passing through this loop, \"tables\" pipeline parameter must be casted to json when needed.\n",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "filter_control_column_type_2_db_datetime_notpart",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_if_not_partitioned_incremental').output.value\n",
						"type": "Expression"
					},
					"isSequential": false,
					"batchCount": 4,
					"activities": [
						{
							"name": "1_load_delta_incremental_table",
							"description": "Loading the incremental delta requires a lowerbound and an upperbound. \nUpperbound was generated by the lookup activity \"1_get_max_control_column_in_source_db\". And lowerbound was generated by the lookup activity \"1_get_watermark_row_for_table\".\n\nIn order to be able to receive control column as integer or datetimes, we generalized converting to timestamp, then to string and then to int. The join operation between the table and control_table will be performed using the specified join column as the key. As a result we'll get a copy of all the columns specified in 'columns' parameter, the join column and the control column.\n\nWe had so many trouble with some string columns, so we decided to use REGEX and TRIM to ensure the we trim all the characters that aren't STRICTLY NUMBERS or elements of datetime such as \":\", \"/\", \"-\" and whitespace.\n\nWARNING: All the limits must be inclusive, that is, the the control column should be => lowerbound and <= upperbound!! It's possible that we are getting redundant compared to the last load, because all the limits are inclusive. Don't worry, it will be considered in the processing pipeline with Spark.",
							"type": "Copy",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 2,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlServerSource",
									"sqlReaderQuery": {
										"value": "SELECT TD.@{json(item()).join_column},@{json(item()).columns},@{json(item()).control_column}\nFROM @{json(item()).schema}.@{json(item()).table} TD \n@{json(item()).join_type} JOIN @{json(item()).schema}.@{json(item()).control_table} TC \nON TD.@{json(item()).join_column} = TC.@{json(item()).join_column} \nWHERE \nCOALESCE(CONVERT(BIGINT,@{pipeline().globalParameters.mssqlserver__function__regexp_replace}(CONVERT(varchar(max), @{json(item()).control_column}, 121),'^0-9','@{json(item()).control_column_mask_value}')), CAST('@{pipeline().parameters.increment.control_column.lowerbound}' AS BIGINT))\n>= CAST('@{pipeline().parameters.increment.control_column.lowerbound}' AS BIGINT) \nAND \nCOALESCE(CONVERT(BIGINT,@{pipeline().globalParameters.mssqlserver__function__regexp_replace}(CONVERT(varchar(max), @{json(item()).control_column}, 121),'^0-9','@{json(item()).control_column_mask_value}')), CAST('@{pipeline().parameters.increment.control_column.lowerbound}' AS BIGINT))\n<= @{pipeline().parameters.increment.control_column.upperbound}",
										"type": "Expression"
									},
									"queryTimeout": "02:00:00",
									"isolationLevel": "ReadCommitted",
									"partitionOption": "None"
								},
								"sink": {
									"type": "ParquetSink",
									"storeSettings": {
										"type": "AzureBlobFSWriteSettings"
									},
									"formatSettings": {
										"type": "ParquetWriteSettings"
									}
								},
								"enableStaging": false,
								"enableSkipIncompatibleRow": true,
								"redirectIncompatibleRowSettings": {
									"linkedServiceName": {
										"referenceName": "cnibigdatadlsgen2stg",
										"type": "LinkedServiceReference"
									},
									"path": {
										"value": "@{pipeline().parameters.container}/@{pipeline().parameters.dls.folders.error}/dbo/@{json(item()).schema}/@{json(item()).table}",
										"type": "Expression"
									}
								}
							},
							"inputs": [
								{
									"referenceName": "sqlserver_parametrized",
									"type": "DatasetReference"
								}
							],
							"outputs": [
								{
									"referenceName": "adls_parameterized_partitioned_source",
									"type": "DatasetReference",
									"parameters": {
										"container": {
											"value": "@pipeline().parameters.container",
											"type": "Expression"
										},
										"url": {
											"value": "@pipeline().parameters.url",
											"type": "Expression"
										}
									}
								}
							]
						},
						{
							"name": "1_sqldw_healthy_check",
							"description": "Check if SQLDW is up and healthy in order to be able to write in watermark table used later in the pipeline. It prevents us to save data without refreshing the watermark table.",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "1_run_raw_notebook",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "sqldw_healthy_check",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true
							}
						},
						{
							"name": "1_update_watermark_row_for_table",
							"description": "Calls the storage procedure that updates the control_column_value in SQLDW to upperbound value from activity '1_get_max_control_column_in_source_db'.\nIt will ensure that the next incremental load will get only the records from this value.\n\n",
							"type": "SqlServerStoredProcedure",
							"dependsOn": [
								{
									"activity": "1_sqldw_healthy_check",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 2,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"storedProcedureName": {
									"value": "@pipeline().parameters.watermark.procedures.update_watermark",
									"type": "Expression"
								},
								"storedProcedureParameters": {
									"ControlColumnValue": {
										"value": {
											"value": "@{string(pipeline().parameters.increment.control_column.upperbound)}",
											"type": "Expression"
										},
										"type": "String"
									},
									"TableName": {
										"value": {
											"value": "dbo.@{toLower(json(item()).schema)}.@{toLower(json(item()).table)}",
											"type": "Expression"
										},
										"type": "String"
									}
								}
							},
							"linkedServiceName": {
								"referenceName": "cnibigdatasqldw",
								"type": "LinkedServiceReference"
							}
						},
						{
							"name": "1_run_raw_notebook",
							"description": "Invokes the pipeline for running Databricks",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "1_load_delta_incremental_table",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__4__run_databricks_notebook",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"databricks": {
										"value": "@pipeline().parameters.databricks",
										"type": "Expression"
									},
									"adf": {
										"value": "@pipeline().parameters.adf",
										"type": "Expression"
									},
									"dls": {
										"value": "@pipeline().parameters.dls",
										"type": "Expression"
									},
									"table": {
										"value": "@string(item())",
										"type": "Expression"
									}
								}
							}
						}
					]
				}
			}
		],
		"parameters": {
			"db": {
				"type": "object"
			},
			"tables": {
				"type": "array"
			},
			"dls": {
				"type": "object"
			},
			"watermark": {
				"type": "object"
			},
			"databricks": {
				"type": "object"
			},
			"adf": {
				"type": "string"
			},
			"increment": {
				"type": "object"
			},
			"container": {
				"type": "string"
			},
			"url": {
				"type": "string"
			}
		},
		"folder": {
			"name": "templates/raw/bdo/mssqlserver"
		},
		"annotations": [
			"template",
			"raw",
			"sqlserver"
		]
	},
	"type": "Microsoft.DataFactory/factories/pipelines"
}