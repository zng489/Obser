{
	"name": "oracle_import_incremental__1__loader",
	"properties": {
		"description": "Well, this one uses a Filter condition for making things work for incremental loads.\n\nWARNING: \"tables\" object must be type ARRAY.\nAll objects in this array must be, in reality STRING type, enclosed by \".\nInside this objects, you should enclose everything in SINGLE QUOTES.\nOtherwise, things are not going to work. I warned you!\n\nHere's an example:\n[\"{'schema': 'INDDESEMPENHO', 'table':'ESTABELECIMENTO','load_type':'incremental','partition_column':'DATAATUALIZACAO','partitions':5,'control_column':'DATAATUALIZACAO','control_column_type_2_db':'datetime', 'control_column_default_value': '19000101',\n'control_column_mask_value': 'DD/MM/YYYY HH24:MI:SS'}\"]\n\n",
		"activities": [
			{
				"name": "filter_control_column_type_2_db_int",
				"description": "Filtering control column type as int, if parameter \"control_column_type_2_db\" in table dictionary is integer and sequential.",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "filter_if_partition_columns_eq_control_column",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_if_partition_columns_eq_control_column').output.value",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).control_column_type_2_db), 'int')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "filter_control_column_type_2_db_datetime",
				"description": "Filtering control column type as int, if parameter \"control_column_type_2_db\" in table dictionary is datetime.\n\nThis parameter \"control_column_type_2_db\" isn't the original type from database, but rather the behavior of the data. So it could be a string, int or datetime. It will work only if the column represents a datetime format.",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "filter_if_partition_columns_eq_control_column",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_if_partition_columns_eq_control_column').output.value",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).control_column_type_2_db), 'datetime')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "filter_control_column_type_2_db_string",
				"description": "Filtering control column type as string, if parameter \"control_column_type_2_db\" in table dictionary is string",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "filter_if_partition_columns_eq_control_column",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_if_partition_columns_eq_control_column').output.value",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).control_column_type_2_db), 'string')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "filter_if_partitioned_incremental",
				"description": "Checks if the partition column parameter is null. If not null, the load will be partitioned from this column.",
				"type": "Filter",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@pipeline().parameters.tables",
						"type": "Expression"
					},
					"condition": {
						"value": "@not(equals(toLower(json(item()).partition_column), 'null'))",
						"type": "Expression"
					}
				}
			},
			{
				"name": "filter_if_not_partitioned_incremental",
				"description": "Checks if the partition column parameter is null. If null, table will be loaded without partitioning",
				"type": "Filter",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@pipeline().parameters.tables",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).partition_column), 'null')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "filter_if_partition_columns_eq_control_column",
				"description": "Checks if partition column is the same as the control column defined on the parameters as 'partition_column' and 'control_column'",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "filter_if_partitioned_incremental",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_if_partitioned_incremental').output.value",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).partition_column), toLower(json(item()).control_column))",
						"type": "Expression"
					}
				}
			},
			{
				"name": "filter_if_partition_columns_noteq_control_column",
				"description": "Checks if partition column is distinct from the control column defined on the parameters as 'partition_column' and 'control_column'",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "filter_if_partitioned_incremental",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_if_partitioned_incremental').output.value",
						"type": "Expression"
					},
					"condition": {
						"value": "@not(equals(toLower(json(item()).partition_column), toLower(json(item()).control_column)))",
						"type": "Expression"
					}
				}
			},
			{
				"name": "1_for_each_incremental_control_column_datetime",
				"description": "This makes our pipeline generic and iterable. After filtering the incremental tables that have the control column that behaves like a datetime and you want to load all the data until the current date (not by year), we shall proceed working with on any of them . \n\nAfter passing through this loop, \"tables\" pipeline parameter must be casted to json when needed.\n\nWARNING: We tested this pipeline only for the table INDDESEMPENHO.ESTABELECIMENTO that has the control column of datetime type. It has not been already tested for a case in which the control column has a integer type that behaves like a datetime. It should work, but we don't have a test case yet.\n\n",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "filter_control_column_type_2_db_datetime",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_control_column_type_2_db_datetime').output.value",
						"type": "Expression"
					},
					"isSequential": false,
					"batchCount": 4,
					"activities": [
						{
							"name": "1_load_delta_incremental_table_partitioned",
							"description": "Loading the incremental delta requires a lowerbound and an upperbound. \nUpperbound was generated by the lookup activity \"1_get_max_control_column_in_source_db\". And lowerbound was generated by the lookup activity \"1_get_watermark_row_for_table\".\nFor this case, the we'll partition the load by the control column.\n\nIn order to be able to receive control column as integer or datetimes, we generalized converting to timestamp, then to string and then to int.\n\nWe had so many trouble with some string columns, so we decided to use REGEX and TRIM to ensure the we trim all the characters that aren't STRICTLY NUMBERS or elements of datetime such as \":\", \"/\", \"-\" and whitespace.\n\nWARNING: All the limits must be inclusive, that is, the the control column should be => lowerbound and <= upperbound!! It's possible that we are getting redundant compared to the last load, because all the limits are inclusive. Don't worry, it will be considered in the processing pipeline with Spark.\n\nRemember to change the sink dataset to \"adls_parametrized_partitioned_source_v2\" when databricks implementation is finished!",
							"type": "Copy",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 1,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "OracleSource",
									"oracleReaderQuery": {
										"value": "SELECT @{json(item()).columns} FROM @{json(item()).schema}.@{json(item()).table} WHERE CAST(NVL(TO_CHAR(TO_TIMESTAMP(TRIM(REGEXP_REPLACE(?AdfRangePartitionColumnName, '[^0-9:\\/\\s-]', '' )), '@{json(item()).control_column_mask_value}'), 'YYYYMMDDHH24MISS' ), '@{pipeline().parameters.increment.control_column.lowerbound}') AS INT) >= ?AdfRangePartitionLowbound AND CAST(NVL(TO_CHAR(TO_TIMESTAMP(TRIM(REGEXP_REPLACE(?AdfRangePartitionColumnName, '[^0-9:\\/\\s-]', '' )), '@{json(item()).control_column_mask_value}'), 'YYYYMMDDHH24MISS' ), '@{pipeline().parameters.increment.control_column.lowerbound}') AS INT) <= ?AdfRangePartitionUpbound",
										"type": "Expression"
									},
									"partitionOption": "DynamicRange",
									"partitionSettings": {
										"partitionColumnName": {
											"value": "@{json(item()).partition_column}",
											"type": "Expression"
										},
										"partitionUpperBound": {
											"value": "@{pipeline().parameters.increment.control_column.upperbound}",
											"type": "Expression"
										},
										"partitionLowerBound": {
											"value": "@{pipeline().parameters.increment.control_column.lowerbound}",
											"type": "Expression"
										}
									}
								},
								"sink": {
									"type": "ParquetSink",
									"storeSettings": {
										"type": "AzureBlobFSWriteSettings"
									},
									"formatSettings": {
										"type": "ParquetWriteSettings"
									}
								},
								"enableStaging": false,
								"parallelCopies": {
									"value": "@json(item()).partitions",
									"type": "Expression"
								},
								"enableSkipIncompatibleRow": true,
								"redirectIncompatibleRowSettings": {
									"linkedServiceName": {
										"referenceName": "cnibigdatadlsgen2stg",
										"type": "LinkedServiceReference"
									},
									"path": {
										"value": "@{pipeline().parameters.container}/@{pipeline().parameters.dls.folders.error}/dbo/@{json(item()).schema}/@{json(item()).table}",
										"type": "Expression"
									}
								}
							},
							"inputs": [
								{
									"referenceName": "oracle_table_parameterized",
									"type": "DatasetReference"
								}
							],
							"outputs": [
								{
									"referenceName": "adls_parameterized_partitioned_source",
									"type": "DatasetReference",
									"parameters": {
										"container": {
											"value": "@pipeline().parameters.container",
											"type": "Expression"
										},
										"url": {
											"value": "@pipeline().parameters.url",
											"type": "Expression"
										}
									}
								}
							]
						},
						{
							"name": "1_run_raw_notebook",
							"description": "Invokes the pipeline for running Databricks",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "1_load_delta_incremental_table_partitioned",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__4__run_databricks_notebook",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"databricks": {
										"value": "@pipeline().parameters.databricks",
										"type": "Expression"
									},
									"adf": {
										"value": "@pipeline().parameters.adf",
										"type": "Expression"
									},
									"dls": {
										"value": "@pipeline().parameters.dls",
										"type": "Expression"
									},
									"table": {
										"value": "@string(item())",
										"type": "Expression"
									}
								}
							}
						},
						{
							"name": "1_raw_load_dbo_unified__5_update_watermark_in_db",
							"description": "Updates the watermark in watermark's database.",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "1_run_raw_notebook",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__5__update_watermark_in_db",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"watermark": {
										"value": "@pipeline().parameters.watermark",
										"type": "Expression"
									},
									"increment": {
										"value": "@pipeline().parameters.increment",
										"type": "Expression"
									},
									"table": {
										"value": "@json(item())",
										"type": "Expression"
									}
								}
							}
						}
					]
				}
			},
			{
				"name": "filter_control_column_type_2_db_datetime_noteq",
				"description": "Filtering control column type as int, if parameter \"control_column_type_2_db\" in table dictionary is datetime, if control column is not equal to partition column.\n\nThis parameter \"control_column_type_2_db\" isn't the original type from database, but rather the behavior of the data. So it could be a string, int or datetime. It will work only if the column represents a datetime format.",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "filter_if_partition_columns_noteq_control_column",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_if_partition_columns_noteq_control_column').output.value",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).control_column_type_2_db), 'datetime')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "2_for_each_incremental_control_column_datetime",
				"description": "This makes our pipeline generic and iterable. After filtering the incremental tables that have the control column that behaves like a datetime and you want to load all the data until the current date (not by year), we shall proceed working with on any of them . \n\nAfter passing through this loop, \"tables\" pipeline parameter must be casted to json when needed.\n\nWARNING: We tested this pipeline only for the table INDDESEMPENHO.ESTABELECIMENTO that has the control column of datetime type. It has not been already tested for a case in which the control column has a integer type that behaves like a datetime. It should work, but we don't have a test case yet.\n\n",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "filter_control_column_type_2_db_datetime_noteq",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_control_column_type_2_db_datetime_noteq').output.value",
						"type": "Expression"
					},
					"isSequential": false,
					"batchCount": 4,
					"activities": [
						{
							"name": "2_get_max_partition_column",
							"description": "This query gets the maximum partition column value that is between the control column's limits.",
							"type": "Lookup",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 2,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "OracleSource",
									"oracleReaderQuery": {
										"value": "SELECT \nMAX(@{json(item()).partition_column}) as max_partition_column \nFROM @{json(item()).schema}.@{json(item()).table}\nWHERE \nCAST(TO_CHAR(TO_TIMESTAMP(TRIM(REGEXP_REPLACE(@{json(item()).control_column}, '[^0-9:\\/\\s-]', '' )), '@{json(item()).control_column_mask_value}'), 'YYYYMMDDHH24MISS' ) AS INT)\n>= \nCAST(@{pipeline().parameters.increment.control_column.lowerbound} AS INT)\nAND \nCAST(TO_CHAR(TO_TIMESTAMP(TRIM(REGEXP_REPLACE(@{json(item()).control_column}, '[^0-9:\\/\\s-]', '' )), '@{json(item()).control_column_mask_value}'), 'YYYYMMDDHH24MISS' ) AS INT)\n<= \nCAST(@{pipeline().parameters.increment.control_column.upperbound} AS INT)",
										"type": "Expression"
									},
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "oracle_table_parameterized",
									"type": "DatasetReference"
								}
							}
						},
						{
							"name": "2_get_min_partition_column",
							"description": "This query gets the minimum partition column value that is between the control column's limits.",
							"type": "Lookup",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 2,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "OracleSource",
									"oracleReaderQuery": {
										"value": "SELECT \nMIN(@{json(item()).partition_column}) as min_partition_column \nFROM @{json(item()).schema}.@{json(item()).table}\nWHERE \nCAST(TO_CHAR(TO_TIMESTAMP(TRIM(REGEXP_REPLACE(@{json(item()).control_column}, '[^0-9:\\/\\s-]', '' )), '@{json(item()).control_column_mask_value}'), 'YYYYMMDDHH24MISS' ) AS INT)\n>= \nCAST(@{pipeline().parameters.increment.control_column.lowerbound} AS INT)\nAND \nCAST(TO_CHAR(TO_TIMESTAMP(TRIM(REGEXP_REPLACE(@{json(item()).control_column}, '[^0-9:\\/\\s-]', '' )), '@{json(item()).control_column_mask_value}'), 'YYYYMMDDHH24MISS' ) AS INT)\n<= \nCAST(@{pipeline().parameters.increment.control_column.upperbound} AS INT)",
										"type": "Expression"
									},
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "oracle_table_parameterized",
									"type": "DatasetReference"
								}
							}
						},
						{
							"name": "check_partition_max_and_min_is_not_null",
							"description": "If max and min for partition_column are null, this menas there's no new records to load. When it happens, just wait and throw complete/success.\n\nIf they are not null, good to go and load as usual. ",
							"type": "IfCondition",
							"dependsOn": [
								{
									"activity": "2_get_max_partition_column",
									"dependencyConditions": [
										"Succeeded"
									]
								},
								{
									"activity": "2_get_min_partition_column",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"expression": {
									"value": "@and(not(equals(coalesce(activity('2_get_min_partition_column').output.firstRow.min_partition_column, 'is_null'), 'isnull')), not(equals(coalesce(activity('2_get_max_partition_column').output.firstRow.max_partition_column, 'is_null'), 'is_null')))",
									"type": "Expression"
								},
								"ifFalseActivities": [
									{
										"name": "load_success",
										"description": "No errors in \"completed\" conditions found. ",
										"type": "Wait",
										"dependsOn": [],
										"userProperties": [],
										"typeProperties": {
											"waitTimeInSeconds": 1
										}
									}
								],
								"ifTrueActivities": [
									{
										"name": "2_load_delta_incremental_table_partitioned",
										"description": "Loading the incremental delta requires a lowerbound and an upperbound. \nUpperbound was generated by the lookup activity \"2_get_max_control_column_in_source_db\". And lowerbound was generated by the lookup activity \"2_get_watermark_row_for_table\".\nFor this case, the we'll partition the load by the control column.\n\nIn order to be able to receive control column as integer or datetimes, we generalized converting to timestamp, then to string and then to int.\n\nWe had so many trouble with some string columns, so we decided to use REGEX and TRIM to ensure the we trim all the characters that aren't STRICTLY NUMBERS or elements of datetime such as \":\", \"/\", \"-\" and whitespace.\n\nWARNING: All the limits must be inclusive, that is, the the control column should be => lowerbound and <= upperbound!! It's possible that we are getting redundant compared to the last load, because all the limits are inclusive. Don't worry, it will be considered in the processing pipeline with Spark.",
										"type": "Copy",
										"dependsOn": [
											{
												"activity": "2_oracle_check_partition_keep_records",
												"dependencyConditions": [
													"Succeeded"
												]
											}
										],
										"policy": {
											"timeout": "7.00:00:00",
											"retry": 2,
											"retryIntervalInSeconds": 30,
											"secureOutput": false,
											"secureInput": false
										},
										"userProperties": [],
										"typeProperties": {
											"source": {
												"type": "OracleSource",
												"oracleReaderQuery": {
													"value": "SELECT @{json(item()).columns} FROM @{json(item()).schema}.@{json(item()).table} WHERE CAST(NVL(TO_CHAR(TO_TIMESTAMP(TRIM(REGEXP_REPLACE(@{json(item()).control_column}, '[^0-9:\\/\\s-]', '' )), '@{json(item()).control_column_mask_value}'), 'YYYYMMDDHH24MISS' ), '@{pipeline().parameters.increment.control_column.lowerbound}') AS INT) >= CAST('@{pipeline().parameters.increment.control_column.lowerbound}' AS INT) AND CAST(NVL(TO_CHAR(TO_TIMESTAMP(TRIM(REGEXP_REPLACE(@{json(item()).control_column}, '[^0-9:\\/\\s-]', '' )), '@{json(item()).control_column_mask_value}'), 'YYYYMMDDHH24MISS' ), '@{pipeline().parameters.increment.control_column.lowerbound}') AS INT) <= CAST(@{pipeline().parameters.increment.control_column.upperbound} AS INT) AND CAST(NVL(?AdfRangePartitionColumnName, @{activity('2_get_min_partition_column').output.firstRow.min_partition_column}) AS INT) >= ?AdfRangePartitionLowbound AND CAST(NVL( ?AdfRangePartitionColumnName, @{activity('2_get_min_partition_column').output.firstRow.min_partition_column}) AS INT)\n <= ?AdfRangePartitionUpbound",
													"type": "Expression"
												},
												"partitionOption": "DynamicRange",
												"partitionSettings": {
													"partitionColumnName": {
														"value": "@{json(item()).partition_column}",
														"type": "Expression"
													},
													"partitionUpperBound": {
														"value": "@{activity('2_get_max_partition_column').output.firstRow.max_partition_column}",
														"type": "Expression"
													},
													"partitionLowerBound": {
														"value": "@{activity('2_get_min_partition_column').output.firstRow.min_partition_column}",
														"type": "Expression"
													}
												}
											},
											"sink": {
												"type": "ParquetSink",
												"storeSettings": {
													"type": "AzureBlobFSWriteSettings"
												},
												"formatSettings": {
													"type": "ParquetWriteSettings"
												}
											},
											"enableStaging": false,
											"parallelCopies": {
												"value": "@json(item()).partitions",
												"type": "Expression"
											},
											"enableSkipIncompatibleRow": true,
											"redirectIncompatibleRowSettings": {
												"linkedServiceName": {
													"referenceName": "cnibigdatadlsgen2stg",
													"type": "LinkedServiceReference"
												},
												"path": {
													"value": "@{pipeline().parameters.container}/@{pipeline().parameters.dls.folders.error}/dbo/@{json(item()).schema}/@{json(item()).table}",
													"type": "Expression"
												}
											}
										},
										"inputs": [
											{
												"referenceName": "oracle_table_parameterized",
												"type": "DatasetReference"
											}
										],
										"outputs": [
											{
												"referenceName": "adls_parameterized_partitioned_source",
												"type": "DatasetReference",
												"parameters": {
													"container": {
														"value": "@pipeline().parameters.container",
														"type": "Expression"
													},
													"url": {
														"value": "@pipeline().parameters.url",
														"type": "Expression"
													}
												}
											}
										]
									},
									{
										"name": "2_oracle_check_partition_keep_records",
										"description": "Invokes the pipeline that checks for data loss when considering double filtering of for control_column and partition ranges.\n\ncontrol_column range is casted INT.\n",
										"type": "ExecutePipeline",
										"dependsOn": [],
										"userProperties": [],
										"typeProperties": {
											"pipeline": {
												"referenceName": "oracle_check_partition_keep_records",
												"type": "PipelineReference"
											},
											"waitOnCompletion": true,
											"parameters": {
												"table": {
													"value": "@json(item())",
													"type": "Expression"
												},
												"query": {
													"value": "{\"filters\":{\"control_column\":{\"lowerbound\":@{pipeline().parameters.increment.control_column.lowerbound},\"upperbound\":@{pipeline().parameters.increment.control_column.upperbound}} ,\"partition_column\":{\"lowerbound\":@{activity('2_get_min_partition_column').output.firstRow.min_partition_column},\"upperbound\":@{activity('2_get_max_partition_column').output.firstRow.max_partition_column}}}}",
													"type": "Expression"
												},
												"db": {
													"value": "@pipeline().parameters.db",
													"type": "Expression"
												},
												"adf": {
													"value": "@pipeline().parameters.adf",
													"type": "Expression"
												},
												"watermark": {
													"value": "@pipeline().parameters.watermark",
													"type": "Expression"
												}
											}
										}
									},
									{
										"name": "2_run_raw_notebook",
										"description": "Invokes the pipeline for running Databricks",
										"type": "ExecutePipeline",
										"dependsOn": [
											{
												"activity": "2_load_delta_incremental_table_partitioned",
												"dependencyConditions": [
													"Succeeded"
												]
											}
										],
										"userProperties": [],
										"typeProperties": {
											"pipeline": {
												"referenceName": "raw_load_dbo_unified__4__run_databricks_notebook",
												"type": "PipelineReference"
											},
											"waitOnCompletion": true,
											"parameters": {
												"databricks": {
													"value": "@pipeline().parameters.databricks",
													"type": "Expression"
												},
												"adf": {
													"value": "@pipeline().parameters.adf",
													"type": "Expression"
												},
												"dls": {
													"value": "@pipeline().parameters.dls",
													"type": "Expression"
												},
												"table": {
													"value": "@string(item())",
													"type": "Expression"
												}
											}
										}
									},
									{
										"name": "2_raw_load_dbo_unified__5_update_watermark_in_db",
										"description": "Updates the watermark in watermark's database.",
										"type": "ExecutePipeline",
										"dependsOn": [
											{
												"activity": "2_run_raw_notebook",
												"dependencyConditions": [
													"Succeeded"
												]
											}
										],
										"userProperties": [],
										"typeProperties": {
											"pipeline": {
												"referenceName": "raw_load_dbo_unified__5__update_watermark_in_db",
												"type": "PipelineReference"
											},
											"waitOnCompletion": true,
											"parameters": {
												"watermark": {
													"value": "@pipeline().parameters.watermark",
													"type": "Expression"
												},
												"increment": {
													"value": "@pipeline().parameters.increment",
													"type": "Expression"
												},
												"table": {
													"value": "@json(item())",
													"type": "Expression"
												}
											}
										}
									}
								]
							}
						}
					]
				}
			},
			{
				"name": "filter_control_column_type_2_db_datetime_notpart",
				"description": "Filtering control column type as int, if parameter \"control_column_type_2_db\" in table dictionary is datetime.\n\nThis parameter \"control_column_type_2_db\" isn't the original type from database, but rather the behavior of the data. So it could be a string, int or datetime. It will work only if the column represents a datetime format.",
				"type": "Filter",
				"dependsOn": [
					{
						"activity": "filter_if_not_partitioned_incremental",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_if_not_partitioned_incremental').output.value\n",
						"type": "Expression"
					},
					"condition": {
						"value": "@equals(toLower(json(item()).control_column_type_2_db), 'datetime')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "3_for_each_incremental_control_column_datetime",
				"description": "This makes our pipeline generic and iterable. After filtering the incremental tables that have the control column that behaves like a datetime and you want to load all the data until the current datetime (not by year), we shall proceed working with on any of them . \n\nAfter passing through this loop, \"tables\" pipeline parameter must be casted to json when needed.\n\nWARNING: We tested this pipeline only for the table INDDESEMPENHO.ESTABELECIMENTO that has the control column of datetime type. It has not been already tested for a case in which the control column has a integer type that behaves like a datetime. It should work, but we don't have a test case yet.\n\n",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "filter_control_column_type_2_db_datetime_notpart",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('filter_if_not_partitioned_incremental').output.value\n",
						"type": "Expression"
					},
					"isSequential": false,
					"batchCount": 4,
					"activities": [
						{
							"name": "3_load_delta_incremental_table",
							"description": "Loading the incremental delta requires a lowerbound and an upperbound. \nUpperbound was generated by the lookup activity \"1_get_max_control_column_in_source_db\". And lowerbound was generated by the lookup activity \"1_get_watermark_row_for_table\".\n\nIn order to be able to receive control column as integer or datetimes, we generalized converting to timestamp, then to string and then to int.\n\nWe had so many trouble with some string columns, so we decided to use REGEX and TRIM to ensure the we trim all the characters that aren't STRICTLY NUMBERS or elements of datetime such as \":\", \"/\", \"-\" and whitespace.\n\nWARNING: All the limits must be inclusive, that is, the the control column should be => lowerbound and <= upperbound!! It's possible that we are getting redundant compared to the last load, because all the limits are inclusive. Don't worry, it will be considered in the processing pipeline with Spark.",
							"type": "Copy",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 2,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "OracleSource",
									"oracleReaderQuery": {
										"value": "SELECT @{json(item()).columns} FROM @{json(item()).schema}.@{json(item()).table} WHERE CAST(NVL(TO_CHAR(TO_TIMESTAMP(TRIM(REGEXP_REPLACE(@{json(item()).control_column}, '[^0-9:\\/\\s-]', '' )), '@{json(item()).control_column_mask_value}'), 'YYYYMMDDHH24MISS' ), '@{pipeline().parameters.increment.control_column.lowerbound}') AS INT) >= CAST('@{pipeline().parameters.increment.control_column.lowerbound}' AS INT) AND CAST(NVL(TO_CHAR(TO_TIMESTAMP(TRIM(REGEXP_REPLACE(@{json(item()).control_column}, '[^0-9:\\/\\s-]', '' )), '@{json(item()).control_column_mask_value}'), 'YYYYMMDDHH24MISS' ), '@{pipeline().parameters.increment.control_column.lowerbound}') AS INT) <= @{pipeline().parameters.increment.control_column.upperbound}",
										"type": "Expression"
									},
									"partitionOption": "None",
									"queryTimeout": "03:20:00"
								},
								"sink": {
									"type": "ParquetSink",
									"storeSettings": {
										"type": "AzureBlobFSWriteSettings"
									},
									"formatSettings": {
										"type": "ParquetWriteSettings"
									}
								},
								"enableStaging": false,
								"enableSkipIncompatibleRow": true,
								"redirectIncompatibleRowSettings": {
									"linkedServiceName": {
										"referenceName": "cnibigdatadlsgen2stg",
										"type": "LinkedServiceReference"
									},
									"path": {
										"value": "@{pipeline().parameters.container}/@{pipeline().parameters.dls.folders.error}/dbo/@{json(item()).schema}/@{json(item()).table}",
										"type": "Expression"
									}
								}
							},
							"inputs": [
								{
									"referenceName": "oracle_table_parameterized",
									"type": "DatasetReference"
								}
							],
							"outputs": [
								{
									"referenceName": "adls_parameterized_partitioned_source",
									"type": "DatasetReference",
									"parameters": {
										"container": {
											"value": "@pipeline().parameters.container",
											"type": "Expression"
										},
										"url": {
											"value": "@pipeline().parameters.url",
											"type": "Expression"
										}
									}
								}
							]
						},
						{
							"name": "3_run_raw_notebook",
							"description": "Invokes the pipeline for running Databricks",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "3_load_delta_incremental_table",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__4__run_databricks_notebook",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"databricks": {
										"value": "@pipeline().parameters.databricks",
										"type": "Expression"
									},
									"adf": {
										"value": "@pipeline().parameters.adf",
										"type": "Expression"
									},
									"dls": {
										"value": "@pipeline().parameters.dls",
										"type": "Expression"
									},
									"table": {
										"value": "@string(item())",
										"type": "Expression"
									}
								}
							}
						},
						{
							"name": "3_raw_load_dbo_unified__5_update_watermark_in_db",
							"description": "Updates the watermark in watermark's database.",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "3_run_raw_notebook",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "raw_load_dbo_unified__5__update_watermark_in_db",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"watermark": {
										"value": "@pipeline().parameters.watermark",
										"type": "Expression"
									},
									"increment": {
										"value": "@pipeline().parameters.increment",
										"type": "Expression"
									},
									"table": {
										"value": "@json(item())",
										"type": "Expression"
									}
								}
							}
						}
					]
				}
			}
		],
		"parameters": {
			"db": {
				"type": "object"
			},
			"tables": {
				"type": "array"
			},
			"dls": {
				"type": "object"
			},
			"watermark": {
				"type": "object"
			},
			"databricks": {
				"type": "object"
			},
			"adf": {
				"type": "object"
			},
			"increment": {
				"type": "object"
			},
			"container": {
				"type": "string"
			},
			"url": {
				"type": "string"
			}
		},
		"folder": {
			"name": "templates/raw/bdo/oracle"
		},
		"annotations": []
	},
	"type": "Microsoft.DataFactory/factories/pipelines"
}